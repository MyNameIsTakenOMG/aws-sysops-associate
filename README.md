# aws-sysops-associate

## Table of Content

- [EC2 for SysOps](#ec2-for-sysops)
- [AMI](#ami)
- [Manage EC2 at scale System manager](#manage-ec2-at-scale-system-manager)
- [High availability and scalability](#high-availability-and-scalability)
- [Elastic Beanstalk](#elastic-beanstalk)
- [Cloudformation](#cloudformation)
- [Lambda](#lambda)
- [EC2 storage and data management](#ec2-storage-and-data-management)
- [S3](#s3)
- [S3 advanced](#s3-advanced)
- [S3 security](#s3-security)
- [Advanced storage solutions](#advanced-storage-solutions)
- [Cloudfront](#cloudfront)
- [Databases](#databases)
- [Monitoring and audit and performance](#monitoring-and-audit-and-performance)
- [Account management](#account-management)
- [Disaster recovery](#disaster-recovery)
- [Security and compliance](#security-and-compliance)
- [Identity](#identity)
- [VPC](#vpc)
- [Route 53](#route-53)
- [Other services](#other-services)
- [Tests reviews](#tests-reviews)


## EC2 for SysOps

- changing instance types (only for EBS-backed instances): stop the instance, apply change, then restart the instance
- enhanced networking:
  - EC2 enhanced networking(sr-iov): elastic network adapter(higher bandwidth)
  - Elastic fabric adapter: improved ENA, `only works for linux`, great for tightly coupled workloads
- placement groups:
  - cluster: cluster instances into a low-latency group in a AZ, if AZ fails, all instances will fail
  - spread: 7 instances per group per AZ (critical apps)
  - partition: up to 7 partitions per AZ, up to 100 instances per group
- shutdown behavior & termination protection
  - not from `aws console` or `aws cli`, but `$shutdown` from within the ec2 system, then stop(default) or terminate will be performed(two options)
  - termination protection `only protect against termination of aws console or aws cli`
- ec2 launch troublshooting
  - **InstanceLimitExceeded**: means that you have reached your limit of max number of vCPUs per region
    - Note: vCPU-based limits only apply to running On-Demand instances and Spot instances
  - **InsufficientInstanceCapacity**: means AWS does not have that enough On-Demand capacity in the particular AZ where the instance is launched.
  - **InstanceTerminates Immediately**: goes from pending to terminated
    - reach EBS volume limit
    - EBS snapshot is corrupt
    - root EBS is encrypted, but you have no permission to decrypt it
    - AMI is missing some required part
- ec2 ssh troubleshooting
  - make sure the permission code `400` for the private key file`.pem`
  - make sure the username for the OS is giving correctly when logging via ssh.
  - `connection time out`: SG/NACL is not configured correctly; check the route table; instance does not have a public ip; cpu load is high...
- ssh vs ec2 instance connect(one time public key)
- ec2 instance purchasing options
  - on-demand instance
  - reserved(1 or 3 years): reserved instances / convertible reserved instances
  - savings plans(1 or 3 years): commitment to an amount of usage
  - spot instances
  - dedicated hosts: compliance requirement or use your own existing server-bound software licenses
  - dedicated instances
  - capacity reservations: combine with reserved or saving plans
- ec2 spot instance requests
  - max spot price: otherwise choose to stop/terminate with a 2 min grace period
  - spot block: (1 - 6 hours) 
  - not for critical jobs or databases
  - cancel a spot request does not terminate instances. So first cancel a spot request, and then terminate instances
- spot fleets: set of Spot Instances + (optional) On-Demand Instances
  - try to meet the target capacity with price constraints
  - allow to automatically request spot instances with lowest price
  - strategies to allocate spot instances:
    - lowestPrices
    - diversified
    - capacityOptimized
    - priceCapacityOptimized
- bustable instances(T2/T3)
  - when a spike of load strikes, cpu can burst using `burst credits`, if all the credits are gone, then cpu will become `BAD`.
  - when a machine stops bursting, then credits are accumulated over time.
  - but if your instances consistently runs low on credits, maybe it is the time to change to a non-burstable instance
- T2/T3 unlimited: extra money, be careful
- Elastic IPs
  - do not pay for it when using it
  - pay for it when not using it
  - can have up to 5 elastic ip
  - should avoid using it
- cloudwatch metrics for ec2
  - aws provided metrics: basic monitoring(default), detail monitoring(pay), no RAM included (CPU, network, status check(instance status, system status, attached EBS status), disk)
  - custom metric 
- unified cloudwatch agent
  - procstat plugin: Collect metrics and monitor system utilization of individual processes. support both linux and windows
- status checks
  - system status checks: aws systems(software and hardware), (`personal health dashboard`), stop and start the instance
  - instance status checks: reboot the instance
  - EBS status checks: reboot the instance or replace EBS
- status checks - cloudwatch metric and recovery
  - cloudwatch alarm: action - recovery
  - auto scaling group
- ec2 hibernate
  - write RAM state into root EBS volume; the EBS volume must be encrypted
  - instance RAM: less than 150 GB
  - available for on-demand, reserved, spot
  - hibernate no more than 60 days



## AMI

- overview:
  - a customization of ec2: configure system, software
  - built for a specific region
  - launch ec2 instances from: public ami, custom ami, or aws marketplace ami
- ami process: start ec2 instance, customize it, stop it, and make an ami, then launch instances from the ami
- ami no-reboot option: by default, aws will shutdown the instance before taking an EBS snapshot and create an AMI. But we can choose no-reboot option, then the system buffer will not be sent to disk before the snapshot is created
- aws backup plans to create ami:
  - aws backup does not reboot the instance when taking ebs snapshot(no-reboot behavior)
  - since it does not stop the instance while taking snapshot, so it is not guaranteed the file system integrity
  - to maintain integrity, you need to provide `reboot` parameter while taking images( eventbridge + lambda + createImage api with reboot)
- cross-account amia sharing: sharing does not affect the ownership of the ami.
  - can only share amis without encrypted volumes or the volumes encrypted with CMK(you must share the CMK as well)
- cross-account ami copy: if you copy the ami shared with you, then you are the new owner of the copy
  - the owner of source ami must give read permission of the ebs snapshot which backs the ami
  - if the backing snapshot is encrypted, then the owner must share the key
- ec2 image builder
  - used to automate the creation, maintain,validate, test of ami
- ami in production
  - force users to only launch ec2 with pre-approved amis(tagged with specific tags) using iam policies(using condition to check tags)
  - combine with aws config to find non-compliant ec2 instances

 

## Manage EC2 at scale System manager

- overview: help manage ec2 and on-prem systems; automation; detection; work for windows and linux; integrated with cloudwatch metrics; integrated with aws config
- SSM agent: installed by default on aws linux ami; make sure the ec2 instances have a proper iam role to allow ssm actions
- aws Tags: a service that can add key-value pairs(tags) to other aws resources(like ec2), which can be used to group or identify resources, automation,... (better to have too many than too few)
- create, view, manage logical group of resources thanks to `Tags`
  - applications, different layers of app stack, prod vs dev env
  - regional service
  - ec2, s3, dynamodb, lambda...
- ssm document: like scripts, but written in json or yml
  - we define parameters, actions
  - also there are existing aws documents
- ssm run-command: execute ssm documents or just a command
  - run command across multiple resources (resource group)
  - rate control/error control
  - integrated with iam, cloudtrail
  - no need for ssh
  - the output can be seen in the console, or sent to s3 or cloudwatch logs
  - send notifications to sns
  - can be invoked using eventbridge
- ssm automation: Simplifies common maintenance and deployment tasks of EC2 instances and other AWS resources
  - Automation runbook: ssm document of type automation
  - pre-defined runbooks(aws) or create custom runbooks
  - can be triggered: console, cli, sdk, eventbridge, on a schedule using `maintenance window`, aws config for rules remediations
- ssm parameter store: secure storage for configuration and secret; notification with aws eventbridge; integrated with cloudformation
- ssm parameter store hierarchy
- ssm parameter store advanced
  - parameter policies: allow to apply a TTL
- ssm inventory
  - collect metadata from your managed instances(ec2/on-prem)
  - metadata: installed software, os driver, configurations, updates,...
  - view data in aws console, or store in s3 and query using athena and quicksight
  - specify the metadata collection interval
  - can create custom inventory
- ssm state manager
  - automate the process of keeping your managed instances (ec2/on-prem) in a state that you define
  - use cases: patch os/software updates on a schedule,...
  - `state manager association`: Defines the state that you want to maintain to your managed instances. Specify a schedule when this configuration is applied
  - Uses SSM Documents to create an Association (e.g., SSM Document to configure CW Agent)
- ssm patch manager: Automates the process of patching managed instances
  - Patch on-demand or on a schedule using Maintenance Windows
  - Scan instances and generate patch compliance report (missing patches)
  - Patch compliance report can be sent to S3
  - patch baseline: Defines which patches should and shouldn’t be installed on your instances
    - pre-define patch baseline: `AWS-RunPatchBaseline (SSM Document)` apply both operating system and application patches 
    - custom patch baseline
  - patch group: Associate a set of instances with a specific Patch Baseline; one patch group with one patch baseline
- ssm maintenance window: Defines a schedule for when to perform actions on your instances
- ssm session manager: Allows you to start a secure shell on your EC2 and on- premises servers. Does not need SSH access, bastion hosts, or SSH keys
  - session logs: sent to s3 or cloudwatch logs


## High availability and scalability

- scalability and high availability
  - scalability: means that an application / system can handle greater loads by adapting. (vertical scalability and horizontal scalability:elasticity)
  - vertical scalability: common for non-ditributed system, such as DB
  - horizontal scalability: means increasing the number of instances / systems for your application. Scaling for distributed system, very common for modern apps
  - high availability: usually goes hand in hand with `horizontal scaling`. Meaning running your application / system in at least 2 data centers (== Availability Zones) in order to survive a data center loss. The high availability can be passive/active.
- load balancing: forward traffic to multiple servers (e.g., EC2 instances) downstream
- why use load balancers: separate public traffic from private traffic; expose a single point of access; apply ssl termination(https);...
- elb: a managed load balancer
- elb: health check (/health) to check if downstream services are available
- elb: alb(layer 7, http,https,websocket), nlb(layer 4 , tcp,tls,udp),gwlb(layer3 network layer-- ip protocol)
- elb: security group
- alb: target group (asg, ecs tasks, lambda functions, ip addresses), health check are at the target group level
- alb: good to know: fixed hostname; the downstream services cannot see the true ip of the client, but we can see it in the header: **X-Forwarded-For**
- nlb: can handle millions of requests, has **one static ip per AZ** and support elastic ip; not in aws free tier
- nlb: target group: ec2 instances, ip addresses, alb, health check: support tcp, http and https
- gwlb: Deploy, scale, and manage a fleet of 3rd party network virtual appliances in AWS. Example: Firewalls, Intrusion Detection and Prevention Systems, Deep Packet Inspection Systems, payload manipulation, ...
- gwlb: uses `GENEVE` protocal on port 6081
- gwlb: target group: ec2 instances, ip addresses(must be private ip)
- sticy session(session affinity): implement stickiness so that the same client is always redirected to the same instance behind a load balancer.
- sticy session--cookie names:
  - application-based cookies: custom cookie(generated by the target), application cookie(generated by the load balancer)
  - duration-based cookies: cookies generated by elb
- cross-zone load balancing:
  - with cross-zone load balancing: each load balancer instance distributes evenly across all registered instances in all AZ
  - without cross-zone load balancing: Requests are distributed in the instances of the node of the Elastic Load Balancer
  - for alb: it is enabled by default, no charges
  - for nlb: disabled by default, will be charged
- SSL(Secure Sockets Layer)/TLS(Transport Layer Security) - basics: An SSL Certificate allows traffic between your clients and your load balancer to be encrypted in transit (in-flight encryption)
- Public SSL certificates are issued by Certificate Authorities (CA). Also, the certificate needs to be renewed
- load balancer -- ssl certficate: clients can use SNI(server name indication to specify the hostname they reach)
- SSL - server name indication: solves the problem of loading multiple ssl certificates onto one web server
- elb -- ssl certificates: alb, nlb support mutliple ssl certificates, clients need to specify SNI
- connection draining:
  - feature naming: `connection draining` for clb, `deregistration delay` for alb, nlb
  - Time to complete “in-flight requests” while the instance is de-registering or unhealthy
- elb health checks:
  - target health status: initial, healthy, unhealthy, unused, draining, unavailable
  - if a target group contains only unhealthy targets, elb routes requests across its unhealthy targets
- elb error code: 200, 4xx, 5xx(503 service unavailable)
- elb monitoring: All Load Balancer metrics are directly pushed to CloudWatch metrics
- elb troubleshooting using metrics:
  - 400 bad requests
  - 503 service unavailable --> check health status of your service in every AZ
  - 504 gateway timeout --> check if `keep-alive` setting on ec2 is enabled
- elb access logs: stored in s3 buckets, encrypted
- alb: not integrated with x-ray yet
- target group settings:
  - deregistration delay
  - slow start: load balancer will gradually increase the number of request sent to the target which gives targets time to warm-up
  - load balancing algorithm type
    - least outstanding requests: The next instance to receive the request is the instance that has the lowest number of pending/unfinished requests(alb, clb)
    - round robin: Equally choose the targets from the target group(alb, clb)
    - flow hash: Selects a target based on the protocol, source/destination IP address, source/destination por t, and TCP sequence number. Each TCP/UDP connection is routed to a single target for the life of the connection. (nlb)
  - stickness enabled; stickness type; stickness app cookie name; stickness app cookie duration; stickness lb cookie duration
- alb -- listener rules: host-headers, http methods, path, source-ip, http headers, query-string
- target group weighting: Specify weight for each Target Group on a single Rule. Allows you to control the distribution of the traffic to your applications
- auto scaling group: scale in/out; re-create ec2 for any unhealthy
  - min, max, desired
- asg attributes: `launch template`, capacity, scaling policies
- auto scaling -- cloudwatch alarm
- asg -- scaling policies:
  - dynamic scaling: target tracking scaling; simple/step scaling
  - scheduled scaling: anticipate the usage pattern
  - predictive scaling
- metrics to scale on: cpu utilization, requests per target, average network in/out, or custom metrics
- asg -- scaling cooldown: After a scaling activity happens, you are in the cooldown period (default 300 seconds). During the cooldown period, the ASG will not launch or terminate additional instances (to allow for metrics to stabilize). **Advice**: Use a ready-to-use AMI to reduce configuration time in order to be serving request fasters and reduce the cooldown period
- asg -- lifecycle hooks: EC2_Instance_Launching, EC2_Instance_Terminating
- launch configuration vs launch template: launch configuration is a legacy. while launch template can have multiple versions, provision on-demand and spot instances, support placement group, can use T2 unlimited burst feature
- asg with sqs: cloudwatch metric - queue length: ApproximateNumberOfMessages, then trigger an alarm and use an alarm action to scale asg
- asg health checks: To make sure you have high availability, means you have least 2 instances running across 2 AZ in your ASG (must configure multi-AZ ASG). health-check: ec2 status, elb status, or custom health checks
- troubleshooting asg issues
  - <number of instances> instance(s) are already running: asg has reached the max capacity
  - Launching EC2 instances is failing: sg or key pair not exist
  - If the ASG fails to launch an instance for over 24 hours, it will automatically suspend the processes (administration suspension)
- cloudwatch metrics for asg:
  - asg-level metrics(opt-in)
  - ec2-level metrics(enabled)
- AWS Auto Scaling: Backbone service of auto scaling for scalable resources in AWS:
  - asg
  - ec2 spot fleet requests
  - ecs
  - dynamodb
  - aurora
- AWS Auto Scaling - scaling plan
  - dynamic scaling
  - predictive scaling



## Elastic Beanstalk

- overview: a developer centric view of deploying an application on AWS. It uses components like ec2, asg, elb, rds,...
  - managed service: handle underlying resources provisioning and configurations, just the application code is the responsibility of the dev
  - we still have full control over the configuration
- elastic beanstalk - components:
  - application: a collection of versions, environments...
  - application version: an iteration of your code
  - environments: collection of aws resources running in an application version
- web server tier vs worker tier
  - web server tier: elb, asg, ec2
  - worker tier: sqs, asg, ec2
- elastic beanstalk deployment mode
  - single instance for dev
  - HA with load balancer for prod

## Cloudformation

- overview: a declarative way of outlining your AWS Infrastructure, for any resources (most of them are supported)
- benefits of cloudformation
  - configurations can be version controlled
  - each resource has a tag within the stack so that you can estimate the cost
  - productivity: declarative, quickly recreate the stack
  - separation of concern: multiple stacks
  - existing templates (documention)
- how cloudformation works
  - templates uploaded to s3 bucket so that cloudformation can reference, then cloudformation will create stacks which will create resources
  - to update a template, we have to upload a new version(cannot edit the current one)
- deploy cloudformation templates
  - manually --> cloudformation console
  - automatically --> edit yml files, and use aws cli or CD tool to deploy your templates
- cloudformation building block
  - template's components:
    - resources: such as `AWS::EC2::Subnet`,
    - parameters: must provide when deploying the template. some options like `AllowedValues`, `Default`, `NoEcho`(boolean). Use **!Ref(Fn::Ref)** to reference the parameters. Also aws has offer some `pseudo parameters` to use
    - mappings: fixed variables. to access map, use **Fn::FindInMap(!FindInMap)**: `!FindInMap [MapName,TopLevelKey, SecondLevelKey]`
    - outputs: The Outputs section declares optional outputs values that we can import into other stacks (if you export them first)! Then later, in other template, we use function `Fn::ImportValue`
    - conditions: used to control the creation of resources or outputs based on a condition (intrinsic function:logical). Conditions can be applied to resources / outputs / etc...
    - aws template format version,...
  - template's helpers: references, functions
- cloudformation -- intrinsic functions
  - Fn::Ref
  - Fn::GetAtt
  - Fn::FindInMap
  - Fn::ImportValue
  - Fn::Base64 (example, ec2 user data)
  - condition functions
- cloudformation rollback: creation/update fails, check the logs
- cloudformation -- service role: `iam:PassRole` permission can be given to the users, who can then allows cloudformation to have enough permissions to work with resources
- cloudformation -- capabilities:
  - CAPABILITY_NAMED_IAM, CAPABILITY_IAM
  - CAPABILITY_AUTO_EXPAND
  - InsufficientCapabilitiesException
- cloudformation deletion policy
  - delete (be aware of s3 bucket)
  - retain
  - snapshot
- cloudformation stack policy: During a CloudFormation Stack update, all update actions are allowed on all resources (default). A Stack Policy is a JSON document that defines the update actions that are allowed on specific resources during Stack updates. Protect resources from unintentional updates.
- cloudformation termination protection: to protect the stack 
- cloudformation -- custom resources
  - used to define resources not supported by aws, define resources that can be outside of cloudformation, custom scripts run during create/update/delete through lambda function
  - service token: specify where cloudformation sends request to, such as lambda, sns
- cloudformation -- dynamic references: Reference external values stored in `Systems Manager Parameter Store` and `Secrets Manager` within CloudFormation templates
  - option 1: ManageMasterUserPassword – creates admin secret implicitly (RDS, Aurora will manage the secret in Secrets Manager and its rotation)
  - option 2: dynamic reference: create a secret, reference the secret in DB resource, then link the secret to the DB instance
- cloudformation -- helper scripts(python scripts come with aws linux ami)
  - Init: A config contains the following and is executed in that order -- packages(packages to download and install), groups, users, sources(download files), files(create files on the ec2), commands(commands to run), services(launch a list of sysvinit)
  - cfn-init: Used to retrieve and interpret the resource metadata, installing packages, creating files and starting services
  - cfn-signal & wait condition:
    - run `cfn-signal` after `cfn-init`
    - define a wait condition to block the template until it receives a signal from `cfn-signal`, we attach a `CreationPolicy` and `Count`
- cloudformation -- nested stacks: best for re-usable configurations, and To update a nested stack, always update the parent (root stack).
- cross stacks vs nested stacks:
  - cross stacks: Helpful when stacks have different lifecycles. Use Outputs Export and Fn::ImportValue. When you need to pass export values to
many stacks (VPC Id...)
  - nested stacks: Helpful when components must be re-used. The nested stack only is important to the higher-level stack (it’s not shared)
- cloudformation -- dependson
  - Applied automatically when using !Ref and !GetAtt
- cloudformation -- stackSets: Create, update, or delete stacks across multiple accounts and regions with a single operation/template
- cloudformation -- stackSet permission models:
  - self-managed permissions: admin account and trusted target accounts
  - service-managed permissions: aws organization
- cloudformation -- troubleshooting
  - delete_failed
  - update_rollback_failed
- cloudformation -- stackSet troubleshooting
  - a stack operation failed, and the stack instance status is OUTDATED: could be insufficient permissions, trying to create a global unique resources(s3), admin account has no trust relationship with target account, reached a limit (service quotas) in target account.


## Lambda

- overview: managed service, auto-scaling, short-execution, run on-demand
- benefits: free tier, easy to increase RAM&CPU, integrated with other services, easy to monitor
- s3 events notification
- lambda execution role: Grants the Lambda function permissions to AWS services / resources
  - When you use an event source mapping to invoke your function, Lambda uses the execution role to read event data.
  - Best practice: create one Lambda Execution Role per function
- lambda resource based policies: Use resource-based policies to give other accounts and AWS services permission to use your Lambda resources
- lambda logging and monitoring
  - cloudwatch logs (make sure lambda has permissions to write logs to the cloudwatch logs)
  - cloudwatch metrics: invocations, durations, errors, throttles, deadlettererrors(failed to send events to dlq), iteratorAge(event source mapping reads from stream), concurrentExecution
- lambda tracing with x-ray: need to enable, and sdk is required
- lambda configuration
  - RAM: from 128MB to 10GB in 1MB increments; The more RAM you add, the more vCPU credits you get; At 1,792 MB, a function has the equivalent of one full vCPU; After 1,792 MB, you get more than one CPU, and need to use multi-threading in your code to benefit from it (up to 6 vCPU)
  - timeout: 3s - 900s
- lambda execution context
  - The execution context is a temporary runtime environment that initializes any external dependencies of your lambda code
  - Great for database connections, HTTP clients, SDK clients...
  - The execution context is maintained for some time in anticipation of another Lambda function invocation
  - The next function invocation can “re-use” the context to execution time and save time in initializing connections objects
  - The execution context includes the /tmp directory
- lambda function /tmp space: 10GB max; use s3 if persistence is required; use KMS if encryption is required
- lambda concurrency and throttling
  - concurrency limit: 1000 (soft limit)
  - reserved concurrency: at function level
  - throttle behavior: synchronous(429 error), asynchronous(retry and retry interval increase exponentially from 1s to 5min, and then go to DLQ)
  - concurrency issue: if do not set reserved concurrency, then other services may get impacted
  - cold start(initialize dependencies, sdk,...) & provision concurrency(concurrency allocated, no cold start): 
- lambda monitoring -- cloudwatch alarms
- lambda monitoring -- cloudwatch logs
- lambda monitoring -- cloudwatch logs insights
  - Collects, aggregates, and summarizes: system-level metrics, diagnostic information

## EC2 storage and data management

- EBS: a network drive(like a network usb key) you can attach to your instances while they run. AZ-scoped, persist data, can only be attached to one instance at a time(CCP level), free tier: 30GB
  - to move an ebs from one az to another, you need snapshot
  - provisioned capacity
- ec2 instance store: ebs has limited performance, while instance store is high-performance hardware disk.
  - better I/O; ephemeral; good for cache, buffer...
- ebs volume types:
  - gp2/gp3: SSD general purpose, Cost effective storage, low-latency. System boot volumes,Virtual desktops, Development and test environments. gp3: IOPS and throughput are independent while gp2 is not
  - io1/io2: SSD high performance. Great for databases workloads
  - st1: HDD low cost HDD
  - sc1: HDD lowest cost
  - **note:** only gp2/gp3 and io1/io2 can be used as boot volume.
- ebs multi-attach -- io1/io2 family
  - Up to 16 EC2 Instances at a time
  - Must use a file system that’s cluster-aware (not
XFS, EXT4, etc...)
- ebs volume resizing
  - can only increase: ebs size, IOPS(io1)
  - after resizing an ebs volume, we need to repartition the drive, and it is possible for the volume to be in the `optimization` phase for a long time, the volume is still usable
  - cannot decrease the size, must create a new one and migrate data
- ebs snapshot: no need to detach the volume to make a snapshot, but recommended. 
- amazon data lifecycle manager: Automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs.
  - Schedule backups, cross-account snapshot copies, delete outdated backups, ...
  - Uses resource tags to identify the resources (EC2 instances, EBS volumes)
  - cannot manage snapshots/ami created outside DLM
  - cannot manage instance store backed ami
- ebs snapshot -- fast snapshot restore(fsr)
  - by default, there is a latency of io when first time fetch snapshot block from s3 buckets
  - solution:  force the initialization of the entire volume(use dd or fio command), or use fsr(create a volume from a snapshot fully initialized, but very expensive)
- ebs snapshot features
  - ebs snapshot archive: archive tier, 75% cheaper, but 24 - 72 hr to restore
  - recycle bin for ebs snapshots: set up rule for retention for the deleted snapshots (1 day to 1 year)
- ebs migration:  az-scoped, to move to another az, create a snapshot and copy to another az, then create a volume out of it
- ebs encryption: all data at rest or in-transit are encrypted, all snapshots are encrypted as well. All are transparent.
- ebs: encrypt an unencrypted volume
  - create a snapshot first
  - encrypt the snapshot(using copy)
  - create a new volume out of the snapshot
  - attach the new volume to the original instance
- efs: managed network file system, can be mounted onto many ec2 instances; support multiple az, highly available, scalable, expensive.
  - compatible with linux system
  - scale: 1000s concurrent nfs client, 10GB+ throughput, grow up to pt-scale network
  - performance mode (set at efs creation time): general purpose(cms,...) / max io(big data...)
  - throughput mode: bursting(1 tb), provisioned(set throughput regardless of storage size), elastic(automatically scales throughput up or down based on your workloads)
- efs storage classes
  - storage tiers: standard(multi-az, good for prod), infrequent access, archive. implement lifecycle policies
  - one zone: good for dev, backup enabled by default
  - over 90% cost-saving
- ebs vs efs
  - ebs: root ebs volume will be terminated by default, but can be disabled
  - efs: can be mounted onto multiple instances across az, only for linux
- efs access point: can manage who can access which files or directories, specify different root directory
- efs operations: lifecycle policies, throughput mode and provisioned mode, access point, or using datasync to migrate data to another efs (could be encrypted)
- efs cloudwatch metrics: percentIOLimit; burstcreditBalance, storageBytes

## S3

- overview: 'infinitely scaling', host static website, integrated with other services
- use cases: backup & restore, DR(disaster recovery), archive, big data analysis,...
- s3 buckets: a regional resource, but its name needs to be unique globally, has naming convention
- s3 objects: every object has a key which is the full path
  - max object size is 5tb, and if uploading more than 5gb, must use `multi-part upload`
  - metadata
  - tags
  - version id(if enabled)
- s3 security:
  - user-based: iam policies
  - resource-based: bucket policies, object access control, bucket access control
  - encryption:
- s3 bucket settings for block public access: These settings were created to prevent company data leaks
- s3 static website hosting
- s3 versioning: enabled at the bucket level
- s3 replication (CRR and SRR):
  - must enable versioning in source and destination buckets
  - cross-region replication
  - same-region replication
  - only new objects will be replicated after enabling replication. Or using s3 batch replication to replicate exsiting objects
  - for delete operation: can replicate delete marker, deletion with a version id are not replicated
  - there is no `chaining` of replication    
- s3 storage classes: (move manually or s3 lifecycle configuration)
  - standard: 99.99% availability
  - standard IA
  - one zone IA
  - glacier instant retrieval: minimal storage duration 90 days, millisecond retrieval, query once a quarter
  - glacier flexible retrieval: (1-5min, 3-5hrs, 5-12hrs), 90 days minimal
  - glacier deep archive: 12 hrs, 48hrs(bulk), duration minimal 180 days
  - intelligent tiering: Small monthly monitoring and auto-tiering fee, no retrieval fee, move objects automatically
- s3 high durability and high availability


## S3 advanced

- s3 moving between storage classes
  - moving objects can be automated using `lifecycle rules`
  - ordering: standard, standard IA, intelligent tier, one-zone IA, glacier instant retrieval, glacier flexible retrieval, glacier deep archive
- s3 lifecycle rule: can be created for a certain prefix, or certain object tags
  - transition actions: configure objects to transition to another storage class
  - expiration actions: configure objects to expire (delete) after some time, can be used to delete incomplete Multi-Part uploads or old versions of objects
- s3 analytics -- storage class analysis: Help you decide when to transition objects to the right storage class
  - Recommendations for Standard and Standard IA: does not work for one-zone IA or glacier
  - Good first step to put together Lifecycle Rules
  - 24-48 to see the data analysis
  - report daily
- s3 event notifications:
  - lambda (resource policy)
  - sqs (resource policy)
  - sns (resource policy)
  - eventbridge (advanced filtering, multi-destinations, eventbridge capabilities)
- s3 baseline performance
  - automatically scale
  - 3500 put/copy/post/delete or 5500 get/head requests per second per prefix in a bucket (tip: instead of using root path of prefix, separate each subpath to have more throughput)
  - no limits to the number of prefixes in a bucket
- s3 performance
  - multi-part upload: recommended for files > 100MB, must use for files > 5GB. parallelize uploading
  - s3 transfer acceleration: upload files to the aws edge locations which will forward to the s3 buckets (compatible with multi-part upload)
  - S3 Byte-Range Fetches: Parallelize GETs by requesting specific byte ranges. Better resilience in case of failures.
    - Can be used to speed up downloads.
    - Can be used to retrieve only partial data (for example the head of a file)
- s3 batch operations
  - work with s3 inventory(get object list), s3 select(filter your objects)
  - A job consists of a list of objects, the action to perform, and optional parameters
- s3 inventory: List objects and their corresponding metadata (alternative to S3 List API
operation)
  - generate daily or weekly report which can be filtered by s3 select
  - the output files: CSV, ORC, or apache parquet which can be queried by athena, redshift, presto, hive, spark,...
- s3 glacier: Low-cost object storage meant for archiving / backup
  - Each item in Glacier is called “Archive” (up to 40TB)
  - Archives are stored in ”Vaults”
- s3 glacier operations
  - vault operations: create&delete, retrieving metadata, download inventory
  - glacier operations: upload, download, delete
  - restore links have an expiry date
  - retrieval options:
    - expedited (1-5 mins)
    - standard (3-5 hrs)
    - bulk (5-12 hrs)
- s3 vault policies and vault lock
  - each vault has one vault access policy(like bucket policy) and vault lock policy( for regulatory and compliance requirements)
- s3 notifications for restore operations
  - vault notification configuration: Configure a vault so that when a job completes, a message is sent to SNS. Optionally, specify an SNS topic when you initiate a job.
  - s3 event notification: S3 supports the restoration of objects archived to S3 Glacier storage classes. Notify when restore job initiated, or notify when restore job is completed
- s3 multi-part upload: (max parts: 10000)
  - Recommended for files > 100MB, must use for files > 5GB 
  - failures: restart the failed parts
  - can use lifecycle policy to automate old parts deletion of unfinished upload after x days
- athena: Serverless query service to analyze data stored in Amazon S3
  - Uses standard SQL language to query the files (built on Presto)
  - Supports CSV, JSON, ORC, Avro, and Parquet
  - work with aws quicksight
- athena performance improvement
  - use `columnar data`: apache parquet or ORC is recommended. use aws glue to convert data to parquet or ORC
  - compress data for smaller retrievals(gzip, bzip2,...)
  - partition datasets for easy querying
  - use larger files (> 128MG) to minimize overhead
- athena federated query: Uses Data Source Connectors that run on AWS Lambda to run Federated Queries (e.g., CloudWatch Logs, DynamoDB, RDS, ...), and then store results back to s3 buckets


## S3 security

- s3 object encryption
  - SSE s3-managed key (default): aes-256 type, must set header: "x-amz-server-side-encryption": "AES256"
  - SSE aws-kms: using kms service, good for user control and audit using cloudtrail. Must set header "x-amz-server-side-encryption": "aws:kms"
    - limits: kms quota: 5500, 10000,30000 requests/s 
  - SSE customer-provided key: https must be used, and customers must store encryption key and provide in every http request header
  - client-side encryption: use client libraries, such as `Amazon S3 Client-Side Encryption Library`. clients must encrypt/decrypt by themselves
- s3 encryption in transit(SSL/TLS)
  - for SSE-C, https is mandatory
- s3  force encryption in transit by using bucket policy( condition: aws:SecureTransport)
- s3 default encryption vs bucket policy
  - NOTE: Bucket Policies are evaluated before “Default Encr yption”
- s3 cors: If a client makes a cross-origin request on our S3 bucket, we need to enable the correct CORS headers. You can allow for a specific origin or for * (all origins)
- s3 MFA delete: To use MFA Delete, Versioning must be enabled on the bucket. Only the bucket owner (root account) can enable/disable MFA Delete
- s3 access logs: Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket. The target logging bucket must be in the same AWS region
  - warning: Do not set your logging bucket to be the monitored bucket, or it will grow exponentially because of a logging loop
- s3 pre-signed url: Generate pre-signed URLs using the S3 Console, AWS CLI or SDK
- s3 glacier vault lock: adopt a write once read many model with a vault lock policy
- s3 object lock(versioning is required)
  - retention mode - compliance
  - retention mode - governance
  - retention period - can be extended
  - legal hold: protect objects independently from retention period
- s3 access point: simplify security management for S3 Buckets
  - each access point has its own dns name
  - access point policy like bucket policy (permissions + path)
  - vpc origin: vpc endpoint(gateway endpoint or interface endpoint) to allow access to the target bucket and its access point
- s3 multi-region access points: bi-directional s3 bucket replication rules, dynamic routing, failover controls
- vpc endpoint gateway for s3
  - for private subnet, use vpc endpoint gateway
  - for public subnet, use internet gateway

## Advanced storage solutions

- aws snow family: Highly-secure, portable devices to collect and process data at the
edge, and migrate data into and out of AWS
- snow family usage process:
  - request snowball device
  - install snowball client on the server
  - connect the snowball to the server and start copying
  - ship the device back to aws
  - data will be loaded to s3 bucket and device will be wiped
- edge computing: processing data at locations where internet is limited or no access to computing power.
  - setup snowball, snowcone to do data process
- aws fsx overview: Fully managed service, Launch 3rd party high-performance file systems on AWS
- aws fsx for windows (file server)
  - support SMB protocal and windows NTFS
  - can be mounted on linux
  - support Microsoft's Distributed File System (DFS) Namespaces
- aws fsx for lustre: The name Lustre is derived from “Linux” and “cluster
  - Machine Learning, High Performance Computing (HPC)
  - Seamless integration with S3 (optional data repository)
  - Can be used from on-premises servers (VPN or Direct Connect)
  - File System Deployment Options: Scratch File System(data is not replicated), Persistent File System(Data is replicated within same AZ)
- aws fsx for NetApp ONTAP: File System compatible with NFS, SMB, iSCSI protocol
  - work with: linux, windows, macos, vmware cloud, ec2, ecs, eks, aws appstream, aws workspaces
  - point-in-time cloning
- aws fsx for OpenZFS: compatible with NFS
  - work with: referencing to `fsx for netapp ontap`
  - point-in-time cloning
- fsx for sysops
  - fsx for windows -- single az: automate replication data within one az. Two generations: Single-AZ 1 (SSD), Single-AZ 2 (SSD & HDD)
  - fsx for windows -- multi az: Automatically replicates data across AZs (synchronous). Standby file server in a different AZ (automatic failover)
- hybrid cloud for storage: S3 is a proprietary storage technology (unlike EFS / NFS), so using aws storage gateway
- aws cloud storage native options:
  - block: ebs, ec2 instance store
  - file: efs, fsx
  - object: s3, glacier
- aws storage gateway: Bridge between on-premises data and cloud data. DR, backup&restore, tiered storage, on-prem cache & low-latency access
  - s3 file gateway: configure using SMB/NFS procotol, lifecycle policy, iam permission, SMB integrated with microsoft AD
  - fsx file gateway: native access to fsx for windows, natively compatible with SMB, NTFS,AD 
  - volume gateway: block storage using iSCSI protocol backed by s3, backed by EBS snapshots. Cached volumes, Stored volumes
  - tape gateway: VirtualTape Library (VTL) backed by Amazon S3 and Glacier
- Storage Gateway – Hardware appliance: Using Storage Gateway means you need on-premises virtualization. Otherwise, you can use a Storage Gateway Hardware Appliance
- storage gateway sysops
  - File Gateway is POSIX compliant (Linux file system)
  - Reboot Storage Gateway VM: (e.g., maintenance)
    - File Gateway: simply restart the Storage GatewayVM
    - Volume and Tape Gateway: Stop Storage Gateway Service; Reboot the Storage Gateway VM; Start Storage Gateway Service
- storage gateway activation
  - get activation key:
    - Using the Gateway VM CLI
    - Make a web request to the GatewayVM (Port 80)
  - troubleshooting:
    - make sure gateway VM has port 80 open
    - check gateway VM has correct time and synchronize it with a Network Time Protocol (NTP) server
- storage gateway -- volume gateway cache
  - cache mode: only the most recent data is stored
  - cache efficiency:
    - Look at the CacheHitPercent metric (you
want it to be high)
    - Look at the CachePercentUsed (you don’t want it to be too high)
  - create a larger cache disk: Use the cached volume to clone a new volume of a larger size and use it as the cached volume
  
## Cloudfront

- overview: Content Delivery Network (CDN). Improves read performance, content is cached at the edge. DDoS protection (because worldwide), integration with Shield, AWS Web Application Firewall
- cloudfron origins
  - s3 buckets: origin access control + s3 bucket policy
  - custom origin(http): alb, ec2, s3 website, any http backend
- cloudfront vs s3 cross-region replication
  - cloudfront: global edge network, files cached for a ttl, great for static content available anywhere
  - s3: read-only, must be setup for each region that needs replication. Great for dynamic content that needs to be available at low-latency in few regions
- cloudfront geo restriction
  - allowlist & blocklist
  - determined by 3rd-party geo-ip DB
- cloudfront access logs: Logs every request made to CloudFront into a logging S3 bucket
  - It’s possible to generate reports on: usage report, viewer report, top referrers report, cache statistics report, popular objects report
  - These reports are based on the data from the Access Logs.
- cloudfront troubleshooting: CloudFront caches HTTP 4xx and 5xx status codes returned by S3 ( or the origin server)
- cloudfront caching
  - based on: headers, session cookies, query string parameters
  - cache lives at the edge location
  - to maximize the cache hit rate to minimize requests to the origin
  - control ttl
  - can invalidate part of cache using the CreateInvalidation API
- cloudfront caching behavior for headers
  - forward all headers to the origin: no caching, ttl=0
  - forward a whitelist of headers: caching based on values in all the specified headers
  - none: Forward only the default headers. no caching based on request headers. Best caching performance
- cloudfront origin headers vs cache behavior
  - origin custom header: origin-level setting, set a header for all requests to origin
  - behavior setting: cache-related setting, contains a whitelist of headers to forward
- cloudfront caching ttl
  - “Cache-Control: max-age” is preferred to “Expires” header
  - If the origin always sends back the header Cache-Control , then you can set the TTL to be controlled only by that header
  - In case you want to set min/max boundaries, you choose “customize” for the Object Caching setting
  - In case the Cache-Control header is missing, it will default to “default value”
- cloudfront cache behavior of cookies(a specific request header)
  - Default: do not process the cookies(Caching is not based on cookies. Cookies are not forwarded)
  - Forward a whitelist of cookies(caching based on values in all the specified cookies)
  - Forward all cookies(Worst caching performance)
- cloudfront cache behavior for query string(Query Strings Parameters are in the URL)
  - Default: do not process the query strings(Caching is not based on query strings. Parameters are not forwarded)
  - Forward a whitelist of query strings(Caching based on the parameter whitelist)
  - Forward all query strings(Caching based on all parameters)
- cloudfront Maximize cache hits by separating static and dynamic distributions
  - for static requests: No headers / session caching rules Required for maximizing cache hits
  - for dynamic: Cache based on correct headers and cookie
- cloudfront increase cache ratio
  - Monitor the CloudWatch metric CacheHitRate
  - Specify how long to cache your objects: Cache-Control max-age header
  - Specify none or the minimally required headers/cookies/query string parameters
  - Separate static and dynamic distributions (two origins)
- cloudfront with alb sticky session
  - Must forward / whitelist the cookie that controls the session affinity to the origin to allow the session affinity to work
  - Set a TTL to a value lesser than when the authentication cookie expires


## Databases

- aws rds(Relational Database Service) overview: managed service, support multiple database engines: postgres, mysql, oracle, microsoft sql server, aurora
  - note: you cannot ssh into the db instance
  - point-in-time-restore, os patching, multi-az for DR, horizontal and vertical scalability,...
- rds storage auto-scaling(when db instance runs out of free storage): can set Maximum storage threshold; useful for applications with unpredictable workloads; support all rds db engines
- rds read replicas for read scalability:
  - only for read
  - up to 15;
  - within az, cross az or cross region;
  - replication is async;
  - replicas can be promoted to their own db
  - apps must update the connection string to leverage read replicas
  - use case: use a replica to do data analysis without affecting the main instance
- rds read replicas network cost: same region free, cross region will be charged
- rds multi az (for Disaster recovery)
  - sync replication
  - acive-passive, standby, one domain name
  - auto-failover
    - condition: the main db instance(os is under software patching, db instance is modified,...), the az is down; a manual failover 
  - note: read replicas can be set as multi az for DR
- rds from single az to multi az
  - zero downtime
  - click `modify` on the db, then a snapshot is taken, then a new db instance is created from it, then the synchronization is established
- lambda and rds:
  - lambda by default is launched in aws public network, it cannot connect to rds. so it is needed to be in the same vpc where the rds is, then an ENI will be created and configured by aws automatically
- rds proxy for aws lambda: The Lambda function must have connectivity to the Proxy (public proxy => public Lambda, private proxy => Lambda in VPC)
  - also, With RDS Proxy, you no longer need code that handles cleaning up idle connections and managing connection pools, which can avoid the issue TooManyConnections
- db parameter groups
  - Dynamic parameters are applied immediately
  - Static parameters are applied after instance reboot
  - Must-know parameter:
    - PostgreSQL / SQL Server: rds.force_ssl=1 => force SSL connections
    - MySQL / MariaDB: require_secure_transport=1 => force SSL connections
- rds backup vs snapshot
  - rds backup: backup is continuous and allow point in time recovery; having retention from 0 to 35 days; to disable backups, set retention to 0
  - snapshot: will stop the db instance for some time when taking snapshots; snapshots taken on multi az will not affect main db; snapshots are incremental
  - rds backup or snapshot will create a new db instance when restoring from them
- rds snapshot sharing
  - manual snapshots: yes
  - automated snapshot: no, needs to be copied first
  - can only share unencrypted snapshots or the ones encrypted by custom managed key(need to share the key as well)
- rds events & event subscription
  - rds keeps records of db instance, snapshots, parameter groups,...
  - subscription: sns
  - rds delivers events to eventbridge
- rds db logs: sent to cloudwatch logs, use metric filter to create metrics and set alarms on them
- rds with cloudwatch
  - cloudwatch metrics
  - enhanced monitoring(gathered from an agent on the db instance) 
- rds performance insight: Visualize your database performance and analyze any issues that affect it
  - With the Performance Insights dashboard, you can visualize the database load and filter the load: waits/sql statements/hosts/users
- aws aurora overview: proprietary db from aws(not open source)
  - Postgres and MySQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or MySQL database)
  - aurora costs more than rds(20%), but more efficient
- aurora high availability and read scaling
  - 6 copies across 3 regions
    - 4 of 6 needed for writes
    - 3 of 6 needed for reads
    - self-healing
  - automated failover
  - up to 15 read replicas
  - support for cross region replication
- aurora db cluster
  - writer endpoint
  - read endpoint
- aurora features: auto-failover; backup & recovery; backtrack: restore data at the any point of time without using backups,...
- backups, backtracking and restores in aurora
  - auto backup: PITR, 1-35 retention, restore to a new db cluster
  - backtracking: only support aurora mysql, do not create a new db cluster, rewind db cluster back and forth 72 hrs
  - aurora db cloning: clone a new db cluster using copy-on-write protocol 
- aurora for sysops
  - You can associate a priority tier (0-15) on each Read Replica
    - Controls the failover priority
    - RDS will promote the Read Replica with the highest priority (lowest tier)
    - If replicas have the same priority, RDS promotes the largest in size
    - If replicas have the same priority and size, RDS promotes arbitrary replica
  - You can migrate an RDS MySQL snapshot to Aurora MySQL Cluster
- aurora cloudwatch metrics: AuroraReplicaLag, DatabaseConnections, ...
- rds & aurora security
  - at-rest encryption: master & replicas use aws kms(defined at the launch time), if master not encrypted, then replicas not either, to encrypt an unencrypted db, go with snapshots
  - in-flight encryption: tls by default
  - security group
  - iam authentication
  - no ssh, except for rds custom
  - audit logs can be enabled and sent to cloudwatch
- elasticache overview: managed service for redis and memcached the same way as rds for relational dbs
- elasticache solution architecture:
  - db cache: Cache must have an invalidation strategy to make sure only the most current data is used in there.
  - user sessions store: backend services or apps write or retrieve user session data from elasticache   
- elasticache: redis vs memcached
  - redis: multi az, read replicas, high availability, data persistence, backup and restore, support sets and sorted sets
  - memcached: multi node partition, no HA, no backup and restore, no persistence, multi-threading architecture
- elasticache replication: cluster mode disabled
  - one shard: one main node, multi replicas nodes(up to 5), async replication, multi az by default for failover
- elasticache replication: cluster mode enabled
  - Data is partitioned across shards (helpful to scale writes)
  - up to 500 nodes per cluster(shards)
- elasticache for redis auto scaling
  - Automatically increase/decrease the desired shards or replicas
  - Supports both Target Tracking and Scheduled Scaling Policies
  - Works only for Redis with Cluster Mode Enabled
- elasticache redis connection endpoints
  - standalone node: one endpoint for read/write
  - cluster mode disabled: primary endpoint(write),reader endpoint(read),node endpoint(read)
  - cluster mode enabled: configuration endpoint(read/write), node endpoint(read)
- redis scaling: cluster mode disabled
  - horizontal: add/remove replicas
  - vertical: create a new node group with different node size
- redis scaling: cluster mode enabled
  - two mode: online scaling; offline scaling
  - horizontal: resharding, shard rebalancing, support online and offline scaling 
  - vertical: change read/write capacity. support online scaling
- redis metrics: evictions(evict LRU items, scale up/out), cpuutilization(scale up/out), swapusage: should not exceed 50 MB (verify there is enough reserved memory)
  - currconnections
  - databaseMemoryUsagepercentage
  - NetworkBytesIn/Out & NetworkPacketsIn/Out
  - ReplicationBytes
  - ReplicationLag
- memcached scaling
  - memcached cluster (1-40 nodes)
  - horizontal:
    - add/remove nodes;
    - auto-discovery allow your app to find nodes automatically. All the cache nodes in the cluster maintain a list of metadata about all other nodes. This is seamless from a client perspective
  - vertical: Memcached clusters/nodes start out empty. scale up process: create a new cluster, update the app to use new cluster endpoint, delete the old cluster
- memcached metrics:
  - evicions
  - cpuutilization
  - swapusage: should not exceed 50 MB
  - currconnections
  - freeablememory

## Monitoring and audit and performance

- cloudwatch metrics overview:
  - metrics is a variable to monitor
  - metrics belong to namespaces
  - dimension is an attribute(or identifier) of a metric
  - metrics have timestamp
- ec2 detailed monitoring (1 min interval, a bit costy)
  - aws free tier allows us to have 10 detailed monitoring metrics
  - Note: EC2 Memory usage is by default not pushed (must be pushed from inside the instance as a custom metric)
- cloudwatch custom metrics
  - example: memory(RAM), use api call putMetricData
  - able to use dimensions to segment metrics
  - metric resolution: standard 1 min, the higher the more costy
  - Important: Accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC2 instance time correctly)
- cloudwatch dashboard
  - global
  - can include graphs from different AWS accounts and regions
- cloudwatch logs
  - log group
  - log stream
  - log expiration
  - cloudwatch logs can send logs to s3(exports), kinesis, lambda, opensearch
  - encrypted by default, can be encrypted using custom key
  - log sources: sdk, cloudwatch agent, elastic beanstalk, ecs, aws lambda, vpc flow logs, api gateway, cloudtrail, route53
- cloudwatch log insights: Search and analyze log data stored in CloudWatch Logs
  - Provides a purpose-built query language
  - Can query multiple Log Groups in different AWS accounts
  - a query engine, not a real-time engine
- cloudwatch logs to s3(export): can take up to 12 hrs, not real-time or near real-time
- cloudwatch logs subcription
  - get a real-time log events from cloudwatch logs for processing and analysis: kinesis( data firehose -> s3), lambda
  - Subscription Filter – filter which logs are events delivered to your destination
  - Cross-Account Subscription – send log events to resources in a different AWS account (KDS, KDF)
- cloudwatch logs aggregation multi-account and multi-region
- cloudwatch alarms
  - various options to define thresholds
  - states: ok, insufficient_data, alarm
  - period: Length of time in seconds to evaluate the metric
  - target: ec2, asg, sns(from which, we can anything we want to do)
  - composite alarms:
    - alarms are on a single metric. Composite Alarms are monitoring the states of multiple other alarms.
    - AND and OR conditions.
    - Helpful to reduce “alarm noise” by creating complex composite alarms
- ec2 instance recovery
  - status check: instance status, system status, ebs status
  - recovery: Same Private, Public, Elastic IP, metadata, placement group
- cloudwatch alarm: good to know
  - alarms can be created based on CloudWatch Logs Metrics Filters
  - To test alarms and notifications, set the alarm state to Alarm using CLI
- cloudwatch synthetics canary
  - configurable script that monitor your APIs, URLs, Websites, ...
  - Reproduce what your customers do programmatically to find issues before customers are impacted
  - integrated with cloudwatch alarm, written in nodejs or python
  - Programmatic access to a headless Google Chrome browser
  - Can run once or on a regular schedule
- CloudWatch Synthetics Canary Blueprints
  - heartbeat monitor
  - api canary
  - broken link checker
  - visual monitoring
  - canary recorder
  - gui workflow builder
- aws eventbridge: schedule(cron job), event pattern(event rules to react events), trigger lambda function, send sqs/sns message,...
- eventbridge event bus:
  - Event buses can be accessed by other AWS accounts using Resource-based Policies
  - You can archive events (all/filter) sent to an event bus (indefinitely or set period)
  - Ability to replay archived events
- eventbridge schema registry:
  - EventBridge can analyze the events in your bus and infer the schema.
  - The Schema Registry allows you to generate code for your application, that will know in advance how data is structured in the event bus
  - schema can be versioned
- eventbridge resource-based policy: Manage permissions for a specific Event Bus
- service quotas cloudwatch alarms: create an alarm on the service quotas console to notify you when you are close to a service quota value threshold
  - Alternative:Trusted Advisor + CW Alarms
    - Limited number of Service Limits checks in Trusted Advisor (~50)
    - Trusted Advisor publishes its check results to CloudWatch
    - You can create CloudWatch Alarms on service quota usage (Service Limits)
- aws cloudtrail(enabled by default)
  - Get an history of events / API calls made within your AWS Account
  - Provides governance, compliance and audit for your AWS Account
  - Can put logs from CloudTrail into CloudWatch Logs or S3
- cloudtrail events:
  - management events:Operations that are performed on resources in your AWS account
  - Data Events: By default, data events are not logged (because high volume operations)
  - CloudTrail Insights Events: Enable CloudTrail Insights to detect unusual activity in your account. CloudTrail Insights analyzes normal management events to create a baseline. And then continuously analyzes write events to detect unusual patterns.
    - cloudtrail console, s3, eventbridge
- cloudtrail events retention:
  - 90 days, could send to s3 using athena to do analysis
- cloudtrail log files integrity validation
  - digest files: References the log files for the last hour and contains a hash of each. store files in s3 bucket
  - help determine if files have been modified since being delivered by cloudtrail
- cloudtrail and eventbridge
  - used to react to any api calls
  - cloudtrail is not real-time:
    - log files to s3: every 5 min
    - event: within 15 min
- cloudtrail organization trail: A trail that will log all events for all AWS accounts in an AWS Organization
  - Member accounts can’t remove or modify the organization trail (view only)
- aws config
  - Helps with auditing and recording compliance of your AWS resources
  - Helps record configurations and changes over time
  - You can receive alerts (SNS notifications) for any changes
  - a per-region service
- config rules
  - aws managed rules(over 75)
  - custom rules
  - rules can be evaluated/triggered when config changes or at regular intervals
  - AWS Config Rules does not prevent actions from happening (no deny)
  - no free tier
- aws config resource
  - View compliance of a resource over time
  - View configuration of a resource over time
  - View CloudTrail API calls of a resource over time
- config rules - remediations
  - Automate remediation of non-compliant resources using SSM Automation Documents
  - Use AWS-Managed Automation Documents or create custom Automation Documents (create a custom automation document to invoke lambda)
  - can set remediation retries
- config rules - notifications
  - using eventbridge to react to events of non-compliant
  - Ability to send configuration changes and compliance state notifications to SNS (all events – use SNS Filtering or filter at client-side)
- config - aggregator
  - The aggregator is created in one central aggregator account
  - If using AWS Organizations, no need for individual Authorization
  - Rules are created in each individual source AWS account
  - Can deploy rules to multiple target accounts using CloudFormation StackSets
- cloudwatch vs cloudtrail vs config
  - cloudwatch: performance monitoring, events, alarm, logging, analysis
  - cloudtrail: record api calls, global service, enabled by default
  - config: Record configuration changes, Evaluate resources against compliance rules,Get timeline of changes and compliance

## Account management

- aws health dashboard
  - service history: Shows all regions, all services health. Shows historical information for each day
  - your account(the ringbell icon): provides aler ts and remediation guidance when AWS is experiencing events that may impact you, which gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources.
    - The dashboard displays relevant and timely information to help you manage events in progress and provides proactive notification to help you plan for scheduled activities.
    - Can aggregate data from an entire AWS Organization
    - Alert, remediation, proactive, scheduled activities
    - global service
  - health event notifications: Use EventBridge to react to changes for AWS Health events in your AWS account( to trigger lambda, sns, sqs, kinesis)
- aws organizations overview:
  - global service
  - can manage multi aws accounts(member accounts) with a management account (main account)
  - single payment method(consolidated billing)
  - shared reserved instances and saving plans discount across accounts
  - pricing benefits from aggregated usage
  - api is available to automate aws account creation
- aws organization -- organizational unit(OU)
  - business units
  - environment units
  - project units
- aws organization advantages and security
  - advantages:
    - multi-accounts vs multi-vpc
    - cloudtrail sends logs to a central s3 buckets
    - cloudwatch sends logs to a central logging account
    - Use tagging standards for billing purposes
    - Establish Cross Account Roles for Admin purposes
  - security: service control policies(scp)   
    - iam policies applied on OUs and member accounts
    - They do not apply to the management account (full admin power)
    - Must have an explicit allow from the root through each OU in the direct path to the target account (does not allow anything by default – like IAM)
- aws organization SCP hierarchy
- aws organization reserved instances:
  - For billing purposes, the consolidated billing feature of AWS Organizations treats all the accounts in the organization as one account, which means all accounts in the organization can receive the hourly cost benefit of Reserved Instances that are purchased by any other account.
  - The payer account (master account) of an organization can turn off Reserved Instance (RI) discount and Savings Plans discount sharing for any accounts in that organization, including the payer account, which means RIs and Savings Plans discounts aren't shared between any accounts that have sharing turned off.
  - To share an RI or Savings Plans discount with an account, both accounts must have sharing turned on.
- aws organization iam policies
  - Use aws:PrincipalOrgID condition key in your resource-based policies to restrict access to IAM principals from accounts in an AWS Organization (like s3 bucket policy)
- aws organization tag policies: Helps you standardize tags across resources in an AWS Organization
  - Ensure consistent tags, audit tagged resources, maintain proper resources categorization, ...
  - Helps with AWS Cost Allocation Tags and Attribute-
based Access Control
  - Prevent any non-compliant tagging operations on specified services and resources (has no effect on resources without tags)
  - Generate a report that lists all tagged/non-compliant resources. And use cloudwatch events to monitor non-compliant tags
- aws control tower: Easy way to set up and govern a secure and compliant multi-account AWS environment based on best practices
  - runs on top of aws organizations and automatically sets up AWS Organizations to organize accounts and implement SCPs (Service Control Policies)
  - benefits:
    - Monitor compliance through an interactive dashboard
    - Detect policy violations and remediate them
    - Automate ongoing policy management using guardrails
    - Automate the set up of your environment in a few clicks
- aws service catalog: users just want a quick self-service portal to launch a set of authorized products pre-defined by admins
  - portfolio: collections of products (cloudformation templates) with iam permissions to control access
  - sharing catalog: Share a reference of the portfolio, then import the shared portfolio in the recipient account (stays in-sync with the original
por tfolio); Deploy a copy of the portfolio into the recipient account (must re-deploy any updates)
  - tagOptions library: Easily manage tags on provisioned products. managed in service catalog. Can be associated with Portfolios and Products
- aws billing alarms
  - Billing data metric is stored in CloudWatch us-east-1
  - Billing data are for overall worldwide AWS costs
  - It’s for actual cost, not for project costs
- cost explorer: Visualize, understand, and manage your AWS costs and usage over time
  - can create custom report
  - Choose an optimal Savings Plan (to lower prices on your bill)
  - Forecast usage up to 12 months based on previous usage
- aws budget: Create budget and send alarms when costs exceeds the budget
  - 4 types of budgets: Usage, Cost, Reservation, Savings Plans
  - Up to 5 SNS notifications per budget
  - Can filter by: az, region, tag, service,...
- cost allocation tags: Use cost allocation tags to track your AWS costs on a detailed level
  - aws generated tags: automatically apply to the resources you created, prefix: aws:
  - user-defined tags: prefix: user:
- cost and usage report: Dive deeper into your AWS costs and usage
  - The AWS Cost & Usage Report contains the most comprehensive set of AWS cost and usage data available.
  - Includes additional metadata about AWS services, pricing, and reservations (e.g., Amazon EC2 Reserved Instances (RIs))
  - Can be configured for daily exports to S3
  - Can be integrated with Athena, Redshift or QuickSight
- aws compute optimizer: Reduce costs and improve performance by recommending optimal AWS resources for your workloads
  - Helps you choose optimal configurations and right- size your workloads (over/under provisioned)
  - Uses Machine Learning to analyze your resources’ configurations and their utilization CloudWatch metrics
  - Recommendations can be exported to S3
  - lower costs up to 25%
  - supported resources: ec2, ec2 asg, ebs, lambda

## Disaster recovery

- aws datasync
  - move large amount of data from/to on-prem to aws (needs agent), or aws-aws (no need agent)
  - can synchronoze to s3, efs, fsx
  - can be scheduled daily/weekly/monthly
  - File permissions and metadata are preserved (NFS POSIX, SMB...)
  - One agent task can use 10 Gbps, can setup a bandwidth limit
- aws backup overview:
  - support PITR for supported services
  - on-demand and scheduled
  - tag-based backup policies
  - You create backup policies known as Backup Plans
    - backup frequency
    - backup window
    - retention period
    - transition to cold storage
  - backup to s3 bucket
- aws backup vault lock: Enforce a WORM (Write Once Read Many) state for all the backups that you store in your AWS Backup Vault
  - Even the root user cannot delete backups when enabled

## Security and compliance

- aws shared responsibility model
  - AWS responsibility - Security of the Cloud: managed services, protecting infra
  - Customer responsibility - Security in the Cloud: network, iam, firewall, encryption,...
  - Shared controls: Patch Management, Configuration Management, Awareness & Training
- DDoS protection on AWS
  - aws shield standard: protects against DDoS attack for your website and applications
  - aws shield advanced: 24/7 premium DDoS protection
  - aws waf: Filter specific requests based on rules
  - cloudfront and route 53: using global edge network
  - Be ready to scale – leverage AWS Auto Scaling
- aws shield
  - standard: free service, provide protection for attacks like SYN/UDP Floods, Reflection attacks and other layer 3/layer 4 attacks
  - advanced: Optional DDoS mitigation service. Protect against more sophisticated attacks on elb, cloudfront, ec2, route53,...
    - Protect against higher fees during usage spikes due to DDoS
    - 24/7 access to AWS DDoS response team (DRP)
- aws waf
  - protects apps from layer 7(http) attacks
  - deploy on alb, cloudfront, api gateway
  - web ACL (access control list)
    - rate-based rules for DDoS
    - Size constraints, geo-match (block countries)
    - Rules can include: IP addresses, HTTP headers, HTTP body, or URI strings
    - Protects from common attack - SQL injection and Cross-Site Scripting (XSS)
- penetration test on aws cloud: AWS customers are welcome to carry out security assessments or penetration tests against their AWS infrastructure without prior approval for 8 services: ec2, rds, aurora, cloudfront, api gateway, lambda and lambda edge functions, lightsail, elastic beanstalk
  - Prohibited Activities: port flooding, protocol flooding, request flooding, DDoS, DNS zone walking via Amazon Route 53 Hosted Zones.
- aws inspector: Automated Security Assessments
  - ec2 instances
  - container images push to the ecr
  - lambda functions
  - reporting & integration with aws security hub
  - send findings to aws eventbridge
  - continuous scanning infra if needed
  - package vulnerability(lambda, ecr, ec2)
  - network reachability(ec2)
  - A risk score is associated with all vulnerabilities for prioritization
- logging in aws security and compliance
  - service logs:
    - cloudtrail trails
    - config rules
    - cloudwatch logs
    - vpc flow logs
    - elb access logs
    - cloudfront logs
    - waf logs
  - logs can be analyzed by athena when they are stored in s3 buckets
  - You should encrypt logs in S3, control access using IAM & Bucket Policies, MFA
  - Move Logs to Glacier for cost savings
- aws guardduty: Intelligent Threat discovery to protect your AWS Account. Uses Machine Learning algorithms, anomaly detection, 3rd party data. no software needs to be installed
  - input data includes:
    - cloudtrail events logs
    - vpc flow logs
    - dns logs
    - other logs(optional): eks audit logs, ebs, lambda,...
  - Can protect against CryptoCurrency attacks (has a dedicated “finding” for it)
  - set up eventbridge rules to be notified by any findings
- aws macie: a managed service, uses machine learning and pattern matching to discover and protect your sensitive data in AWS. Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII) in s3 buckets
  - integrated with eventbridge
- trusted advisor: no software needs to be installed. high level AWS account assessment.
  - recommendation on:
    - cost optimization
    - performance
    - security
    - fault tolerance
    - service limits
    - operational excellence
  - besiness and enterprise support plan: full set of checks, Programmatic Access using AWS Support API
- aws kms overview:
  - aws manages encryption keys for us
  - fully integrated with iam
  - seamlessly integrated with other services: rds, ssm,...
  - Never ever store your secrets in plaintext, especially in your code!
- aws kms keys types:
  - symmetric(AES-256 keys): Single encryption key that is used to Encrypt and Decrypt. AWS services that are integrated with KMS use Symmetric CMKs. You never get access to the KMS Key unencrypted (must call KMS API to use)
  - asymmetric(RSA & ECC key pairs): Public (Encrypt) and Private Key (Decrypt) pair. The public key is downloadable, but you can’t access the Private Key unencrypted
- types of kms keys:
  - AWS Owned Keys (free): SSE-S3, SSE-SQS, SSE-DDB (default key)
  - AWS Managed Key: free (aws/service-name, example: aws/rds or aws/ebs)
  - Customer managed keys created in KMS: $1 / month
  - Customer managed keys imported: $1 / month
- automatic key rotation:
  - AWS-managed KMS Key: automatic every 1 year
  - Customer-managed KMS Key: (must be enabled) automatic & on-demand
  - Imported KMS Key: only manual rotation possible using alias
- kms key policies: similar to s3 bucket policies but the difference is you cannot control access without them.
  - default key policy: access for entire aws account
  - custom kms key policy: define users, roles and who can administer the key. good for cross-account access
- copying snapshot across accounts: do not forget to attach a kms key policy to authorize cross-account access
- kms automatic key rotation
  - AWS-managed KMS Keys: automatically rotated every 1 year
  - For Customer-Managed `Symmetric KMS Key`: key rotation is optional, rotation period is between 90-2560 days, Previous key is kept active so you can decrypt old data, New Key has the same KMS Key ID (only the backing key is changed)
    - on-demand rotation(For Customer-Managed Symmetric KMS Key (not AWS managed CMK)): no need to enable auto-rotation, no need to change existing rotation schedule, limited times of triggering on-demand rotations
- KMS Manual Key Rotation
(Customer-Managed Symmetric KMS Key & Imports):
  - New Key has a different KMS Key ID
  - Keep the previous key active so you can decrypt old data
  - Better to use aliases in this case (to hide the change of key for the application)
  - Good solution to rotate KMS Key that are not eligible for automatic rotation (like asymmetric KMS Key)
- kms alias updating: when doing manual rotation, hidding changes of the key
- Changing The KMS Key For An Encr ypted EBS Volume: to use a new cmk key, create a snapshot, then create and encrypt a new volume using a new cmk key
- Sharing KMS Encrypted RDS DB Snapshots: You can share RDS DB snapshots encrypted with KMS CMK with other accounts, but must first share the KMS CMK with the target account using Key Policy
- kms key deletion consideration
  - Schedule CMK for deletion with a waiting period of 7 to 30 days(pending deletion): during which, the cmk key cannot be used for encryption or be rotated
  - can cancel the key deletion during the waiting period
  - Consider disabling your key instead of deleting it if you’re not sure!
- KMS Key Deletion – CloudWatch Alarm: Use CloudTrail, CloudWatch Logs, CloudWatch Alarms and SNS to be notified when someone tries to use a CMK that’s ”Pending deletion” in a cryptographic operation (Encrypt, Decrypt, ...)
- CloudHSM(Hardware Security Module)
  - KMS => AWS manages the software for encryption
  - CloudHSM => AWS provisions encryption hardware, tamper resistant,
  - You manage your own encryption keys entirely (not AWS)
  - Supports both symmetric and asymmetric encryption (SSL/TLS keys)
  - no free tier
  - Must use the CloudHSM Client Software
  - Redshift supports CloudHSM for database encryption and key management
  - Good option to use with SSE-C encryption
- cloudHSM high availability
  - cloudHSM clusters are spread across multi-az
- cloudHSM integrated with aws service
  - integrated with kms
  - configure kms custom key store with cloudHSM
- aws artifact( not a service): Portal that provides customers with on-demand access to AWS compliance documentation and AWS agreements
  - artifact reports
  - artifact agreements
  - Can be used to support internal audit or compliance
- AWS Certificate Manager (ACM)
  - manage TLS certificates, providing in-flight encryption for websites(https)
  - auto-renew tls certificates
  - support both public(free) and private tls certificates
  - integrated with api gateway, cloudfront, elb
- acm -- request public certificates
  - list domain name
  - select validation method: dns validation(leverage CNAME record), or email validation
  - will take quite some time to verified
  - public certificates will be auto-renewed(60 days before expiry)
- acm -- import public certificates(no auto-renew)
  - acm sends daily expiration events starting 45 days prior to expiration to eventbridge
  - AWS Config has a managed rule named acm-certificate-expiration-check to check for expiring certificates (configurable number of days)
- api gateway endpoints types
  - edge-optimized(default, global): Requests are routed through the CloudFront Edge locations (improves latency), the api gateway still lives in only one region
  - regional: could manually combine with cloudfront distro
  - private
- acm -- integration with api gateway
  - assign a custom domain name to api gateway
  - edge-optimized: TheTLS Certificate must be in the same region as CloudFront, in us-east-1
  - regional: configure tls certificates and import it into api gateway (at the same region)
- SSM parameter store
  - Secure storage for configuration and secrets
  - optionally encrypt using kms
  - serverless, scalable
  - version tracking
  - managed with iam and path
  - notification with cloudwatch events
  - integration with cloudformation
- ssm parameter store hierarchy
- ssm parameter store standard vs advanced tiers
  - parameters policies(advanced tier): apply ttl for parameters to force updating or deleting.
  - Can assign multiple policies at a time
- aws secret manager
  - newer service, able to force rotation of secrets
  - integrated with rds
  - secrets encrypted with kms
  - mostly meant for rds integration
- aws secret manager multi-region secrets
  - Secrets Manager keeps read replicas in sync with the primary Secret
  - Ability to promote a read replica Secret to a standalone Secret
  - Use cases: multi-region apps, disaster recovery strategies, multi-region DB...
- SSM parameter store vs secret manager
  - ssm parameter store: cheaper, no secret rotation(can use eventbridge to make cron job to rotate), kms is optional, integrated with cloudformation
  - secret manager: auto-rotation
  - kms encryption is mandatory
  - can integration with cloudformation 
- secret manager monitoring
  - cloudtrail records api calls
  - cloudtrail captures other related events that might have a security or compliance impact on your AWS account or might help you troubleshoot operational problems.
  - CloudTrail records these events as non-API service events: RotationFailed event, RotationSucceeded event, RotationAbandoned event, CancelSecretVersionDelete event,...
  - Combine with CloudWatch Logs and CloudWatch alarms for automations

## Identity

- iam security tools
  - iam credentials reports(account level)
  - iam access advisor(user level)
- iam access analyzer
  - Find out which resources are shared externally
  - Define Zone of Trust = AWS Account or
AWS Organization
  - Access outside zone of trusts => findings
- Identity Federation
  - lets users outside of AWS to assume temporary role for accessing AWS resources.
  - Federation assumes a form of 3rd party authentication
  - Using federation, you don’t need to create IAM users (user management is outside of AWS)
- SAML Federation For Enterprises
  - To integrate Active Directory / ADFS with AWS (or any SAML 2.0)
  - Provides access to AWS Console or CLI (through temporary creds)
  - No need to create an IAM user for each of your employees
- Custom Identity Broker Application For Enterprises
  - Use only if identity provider is not compatible with SAML 2.0
  - The identity broker must determine the appropriate IAM policy
- AWS Cognito - Federated Identity Pools For Public Applications
  - goal: Provide direct access to AWS Resources from
the Client Side
- AWS STS – Security Token Service
  - Allows to grant limited and temporary access to AWS resources.
  - Token is valid for up to one hour (must be refreshed)
  - AssumeRole: Within your own account: for enhanced security. Cross Account Access: assume role in target account to perform actions there
  - AssumeRoleWithSAML: return credentials for users logged with SAML
  - AssumeRoleWithWebIdentity: return creds for users logged with an IdP (Facebook Login, Google Login, OIDC compatible...). AWS recommends against using this, and using **Cognito** instead
  - GetSessionToken: for MFA, from a user or AWS account root user
- Using STS to Assume a Role
  - Define an IAM Role within your account or cross-account
  - Define which principals can access this IAM Role
  - Use AWS STS (Security Token Service) to retrieve credentials and impersonate the IAM Role you have access to (AssumeRole API)
  - Temporary credentials can be valid between 15 minutes to 1 hour
- Cross-Account Access with STS
- Cognito User Pools (CUP) – User Features: Create a serverless database of user for your web & mobile apps
  - simple login
  - password reset
  - email & phone number verification
  - multi-factor authentication
  - federated identities
  - Login sends back a JSON Web Token (JWT)
- Cognito User Pools (CUP) - Integrations
  - CUP integrates with API Gateway and Application Load Balancer
- Cognito Identity Pools (Federated Identities)
  - Get identities for “users” so they obtain temporary AWS credentials
  - Your identity pool (e.g identity source) can include:
    - Public Providers
    - aws cognito user pool
    - OIDC & SAML
    - Developer Authenticated Identities (custom login server)
    - or guest(unauthenticated)
  - Users can then access AWS services directly or through API Gateway 
- Cognito Identity Pools – IAM Roles
  - Default IAM roles for authenticated and guest users
  - You can partition your users’ access using policy variables
  - Define rules to choose the role for each user based on the user’s ID
  - IAM credentials are obtained by Cognito Identity Pools through STS
  - The roles must have a “trust” policy of Cognito Identity Pools
- Cognito User Pools vs Identity Pools
  - Cognito User Pools (for authentication = identity verification)
  - Cognito Identity Pools (for authorization = access control)
  - CUP + CIP = authentication + authorization

## VPC

- Understanding CIDR – IPv4
  - Classless Inter-Domain Routing – a method for allocating IP addresses
  - Used in Security Groups rules and AWS networking in general
  - Base IP
  - subnet mask
- Public vs. Private IP (IPv4)
  - The Internet Assigned Numbers Authority (IANA) established certain blocks of IPv4 addresses for the use of private (LAN) and public (Internet) addresses
  - Private IP can only allow certain values:
    - 10.0.0.0 – 10.255.255.255 (10.0.0.0/8):  big networks
    - 172.16.0.0 – 172.31.255.255 (172.16.0.0/12): AWS defaultVPC in that range
    - 192.168.0.0 – 192.168.255.255 (192.168.0.0/16): e.g., home networks
  - All the rest of the IP addresses on the Internet are Public
- default vpc walkthrough
  - All new AWS accounts have a default VPC
  - Default VPC has Internet connectivity and all EC2 instances inside it have public IPv4 addresses
  - We also get a public and a private IPv4 DNS names
- vpc in aws -- ipv4
  - max 5 vpc in one region(soft limit)
  - Max. CIDR per VPC is 5, for each CIDR:
    - Min. size is /28 (16 IP addresses)
    - Max. size is /16 (65536 IP addresses)
  - Because VPC is private, only the Private IPv4 ranges are allowed
  - Your VPC CIDR should NOT overlap with your other networks (e.g., corporate)
- VPC – Subnet (IPv4)
  - AWS reserves 5 IP addresses (first 4 & last 1) in each subnet
- Internet Gateway (IGW)
  - Allows resources (e.g., EC2 instances) in a VPC connect to the Internet
  - It scales horizontally and is highly available and redundant
  - Must be created separately from a VPC
  - OneVPC can only be attached to one IGW and vice versa
  - Internet Gateways on their own do not allow Internet access...
  - Route tables must also be edited!
- Bastion Hosts
  - We can use a Bastion Host(public subnet) to SSH into our private EC2 instances
  - Bastion Host security group must allow inbound from the internet on port 22 from restricted CIDR, for example the public CIDR of your corporation
  - Security Group of the EC2 Instances must allow the Security Group of the Bastion Host, or the private IP of the Bastion host
- NAT Instance (outdated, but still at the exam)
  - NAT = Network Address Translation
  - Allows EC2 instances in private subnets to
connect to the Internet
  - the private instances ips are masked by NAT instance(the requests source ip and response destination ip)
  - Must disable EC2 setting: Source / destination Check
  - Must be launched in a public subnet
  - Must have Elastic IP attached to it
  - RouteTables must be configured to route traffic from private subnets to the NAT Instance
  - comments:
    - no HA
    - reached the end of the support in Dec 1, 2020
    - security group and rules must be configured
- NAT gateway
  - aws managed nat, HA
  - created at a specifc az, using an elastic ip
  - cannot be used by ec2 which reside in the same subnet
  - Requires an IGW (Private Subnet => NATGW => IGW)
- NAT gateway -- HA
  - NAT Gateway is resilient within a single Availability Zone
  - Must create multiple NAT Gateways in multiple AZs for fault-tolerance
  - There is no cross-AZ failover needed because if an AZ goes down it doesn't need NAT
- DNS Resolution in VPC
  - DNS Resolution (enableDnsSupport): Decides if DNS resolution from Route 53 Resolver server is supported for theVPC. True (default): it queries the Amazon Provider DNS Server at 169.254.169.253 or the reserved IP address at the base of the VPC IPv4 network range plus two (.2)
  - DNS Hostnames (enableDnsHostnames)
    - By default, it is true, but for newly created vpc, it could be false.
    - Won’t do anything unless enableDnsSupport=true
    - If True, assigns public hostname to EC2 instance if it has a public IPv4
  - If you use custom DNS domain names in a **Private Hosted Zone** in Route 53, you must set both these attributes (enableDnsSupport & enableDnsHostname) to true
- Security Groups & NACLs
  - sg: stateful, instance-level
  - nacl: stateless, subnet-level
- Network Access Control List (NACL)
  - NACL are like a firewall which control traffic from and to subnets
  - One NACL per subnet, new subnets are assigned the Default NACL
  - You define NACL Rules:
    - Rules have a number (1-32766), higher precedence with a lower number
    - First rule match will drive the decision
    - The last rule is an asterisk (*) and denies a request in case of no rule match
    - AWS recommends adding rules by increment of 100
  - Newly created NACLs will deny everything
  - NACL are a great way of blocking a specific IP address at the subnet level
- default NACL
  - Accepts everything inbound/outbound with the subnets it’s associated with
  - Do NOT modify the Default NACL, instead create custom NACLs
- Ephemeral Ports
  - For any two endpoints to establish a connection, they must use ports
  - Clients connect to a defined port, and expect a response on an ephemeral port
  - Different Operating Systems use different port ranges
    - IANA & MSWindows 10: 49152–65535
    - Many Linux Kernels: 32768 – 60999
- NACL with Ephemeral Ports
- VPC – Reachability Analyzer
  - A network diagnostics tool that troubleshoots network connectivity between two endpoints in your VPC(s)
  - It builds a model of the network configuration, then checks the reachability based on these configurations (it doesn’t send packets)
  - When the destination is:
    - Reachable – it produces hop-by-hop details of the virtual network path
    - Not reachable – it identifies the blocking component(s) (e.g., configuration issues in SGs, NACLs, Route Tables, ...)
- VPC peering
  - Privately connect two VPCs using AWS’ network
  - Must not have overlapping CIDRs
  - VPC Peering connection is NOT transitive
(must be established for each VPC that need to communicate with one another)
  - You must update route tables in each VPC’s subnets to ensure EC2 instances can communicate with each other
  - You can create VPC Peering connection between VPCs in different AWS accounts/regions
  - You can reference a security group in a peeredVPC (works cross accounts – same region)
- VPC Endpoints (AWS PrivateLink)
  - Every AWS service is publicly exposed (public URL)
  - VPC Endpoints (powered by AWS PrivateLink) allows you to connect to AWS services using a private network instead of using the public Internet
  - They’re redundant and scale horizontally. They remove the need of IGW, NATGW, ... to access AWS Services
  - in case of issue:
    - Check DNS Setting Resolution in your VPC
    - check route tables
  - types:
    - Interface Endpoints (powered by PrivateLink): charged by hours, data. support most aws services, provisions an ENI as an entry point(must attach a Security Group)
    - Gateway Endpoints: free, support s3 and dynamodb, Provisions a gateway and must be used as a target in a route table (does not use security groups)
- Gateway or Interface Endpoint for S3
  - gateway endpoint is preferable way
  - Interface Endpoint is preferred access is required from on- premises (Site to Site VPN or Direct Connect), a different VPC or a different region
- Lambda in VPC accessing DynamoDB
  - dynamodb is a public service
  - option1: access from the internet
  - option2: access from gateway endpoint(vpc endpoint, free)
    - deploy a vpc gateway endpoint for dynamodb
    - change the route tables
- vpc flow logs
  - Capture information about IP traffic going into your interfaces: vpc flow logs, subnet flow logs, ENI flow logs
  - Helps to monitor & troubleshoot connectivity issues
  - Flow logs data can go to S3, CloudWatch Logs(insight, metric filter with alarm), and Kinesis Data Firehose
  - Captures network information from AWS managed interfaces too: ELB, RDS, ElastiCache, Redshift, WorkSpaces, NATGW, Transit Gateway...
  - syntax:
    - srcaddr & dstaddr
    - srcport & dstport
    - Action – success or failure of the request due to Security Group / NACL
    - Can be used for analytics on usage patterns, or malicious behavior
    - Quer y VPC flow logs using Athena on S3 or CloudWatch Logs Insights
  - Troubleshoot SG & NACL issues( Look at the “ACTION” field ): 
    - Incoming Requests
    - Outgoing Requests
- AWS Site-to-Site VPN
  - Virtual Private Gateway (VGW): VPN concentrator on the AWS side of the VPN connection
  - Customer Gateway (CGW): Software application or physical device on customer side of the VPN connection
  - Site-to-Site VPN Connections
    - Customer Gateway Device (On-premises)
    - `Important step`: enable `Route Propagation` for theVirtual Private Gateway in the route table that is associated with your subnets
    - If you need to ping your EC2 instances from on-premises, make sure you add the ICMP protocol on the inbound of your security groups
- AWS VPN CloudHub
  - Provide secure communication between multiple sites, if you have multiple VPN connections
  - Low-cost hub-and-spoke model for primary or secondary network connectivity between different locations (VPN only)
  - It’s a VPN connection so it goes over the public Internet
  - To set it up, connect multiple VPN connections on the same VGW, setup dynamic routing and configure route tables
- Direct Connect (DX)
  - Provides a **dedicated private** connection from a remote network to your VPC
  - AWS Direct Connect locations: Dedicated connection must be setup between your DC and AWS Direct Connect locations
  - You need to setup aVirtual Private Gateway on yourVPC
  - Access public resources (S3) and private (EC2) on same connection
  - Supports both IPv4 and IPv6
  - Use Cases: Increase bandwidth throughput; Hybrid Environments (on prem + cloud); More consistent network experience
  - Direct Connect Gateway: If you want to setup a Direct Connect to one or more VPC in many different regions (same account), you must use a Direct Connect Gateway
- Direct Connect – Connection Types
  - Dedicated Connections: 1Gbps,10 Gbps and 100 Gbps capacity
  - Hosted Connections: 50Mbps, 500 Mbps, to 10 Gbps
  - Lead times are often longer than 1 month to establish a new connection
- Direct Connect – Encryption
  - Data in transit is not encrypted but is private
  - AWS Direct Connect + VPN provides an IPsec-encrypted private connection
  - Good for an extra level of security, but slightly more complex to put in place
- Direct Connect - Resiliency
  - High Resiliency for Critical Workloads
  - Maximum Resiliency for Critical Workloads: multiple connection between vpcs and aws direct connect locations
- Site-to-Site VPN connection as a backup
  - In case Direct Connect fails, you can set up a backup Direct Connect connection (expensive), or a Site-to-Site VPN connection
- Exposing services in yourVPC to otherVPC
  - option1: make it public
  - option2: vpc peering
- AWS PrivateLink (VPC Endpoint Services)
  - Most secure & scalable way to expose a service to 1000s ofVPC (own or other accounts)
  - Does not require VPC peering, internet gateway, NAT, route tables...
  - Requires a `network load balancer` (Service VPC) and ENI (Customer VPC) or GWLB
  - If the NLB is in multiple AZ, and the ENIs in multiple AZ, the solution is fault tolerant!
- EC2-Classic & AWS ClassicLink (deprecated)
  - EC2-Classic: instances run in a single network shared with other customers
  - ClassicLink allows you to link EC2-Classic instances to a VPC in your account
  - Likely to be distractors at the exam
- Transit Gateway
  - For having transitive peering between thousands of VPC and on-premises, hub-and-spoke (star) connection
  - Regional resource, can work cross-region
  - Share cross-account using Resource Access Manager (RAM)
  - You can peer Transit Gateways across regions
  - Route Tables: limit which VPC can talk with other VPC
  - Works with Direct Connect Gateway,VPN connections
  - Supports IP Multicast (not supported by any other AWS service)
- Transit Gateway: Site-to-Site VPN ECMP
  - ECMP = Equal-cost multi-path routing
  - Routing strategy to allow to forward a packet over multiple best path
  - Use case: create multiple Site- to-Site VPN connections to increase the bandwidth of your connection to AWS
- Transit Gateway: throughput with ECMP
- Transit Gateway – Share Direct Connect between multiple accounts: You can use AWS Resource Access Manager to share Transit Gateway with other accounts.
- VPC – Traffic Mirroring
  - Allows you to capture and inspect network traffic in your VPC
  - Route the traffic to security appliances that you manage
  - Capture the traffic: from(source):ENIs, to(target): an ENI or nlb
  - Capture all packets or capture the packets of your interest (optionally, truncate packets)
  - Source and Target can be in the same VPC or different VPCs (VPC Peering)
  - Use cases: content inspection, threat monitoring, troubleshooting, ...
- IPv6 in VPC
  - Every IPv6 address in AWS is public and Internet-routable (no private range)
  - IPv4 cannot be disabled for your VPC and subnets
  - You can enable IPv6 (they’re public IP addresses) to operate in dual-stack mode
  - Your EC2 instances will get at least a private internal IPv4 and a public IPv6
  - They can communicate using either IPv4 or IPv6 to the internet through an Internet Gateway
- ipv6 troublshooting
  - if you cannot launch an EC2 instance in your subnet, It’s because there are no available IPv4 in your subnet
  - Solution: create a new IPv4 CIDR in your subnet
- Egress-only Internet Gateway
  - Used for IPv6 only
  - (similar to NAT gateway, but for ipv6)
  - You must update the Route Tables
  - because it is egress-only, so it only allows instances to send requests but prevents requests sent from the internet via ipv6
- ipv6 routing
- Networking Costs in AWS per GB - Simplified
  - Use Private IP instead of Public IP for good savings and better network performance
  - Use same AZ for maximum savings (at the cost of high availability)
 - Minimizing egress traffic network cost
   - ingress traffic: typically free
   - egress traffic: outbound traffic
   - Try to keep as much internet traffic within AWS to minimize costs
   - Direct Connect location that are co-located in the same AWS Region result in lower cost for egress network
- S3 Data Transfer Pricing – Analysis for USA
  - ingress: free
  - s3 to cloudfront: free
  - cloudfront to internet is sightly cheaper than s3 to internet
  - s3 cross-region replication
- pricing: NAT gateway vs gateway endpoint
  - for example, ec2 in private network try to connect to s3 bucket
    - option1: nat gateway + internet gateway
    - option2: vpc endpoint (free) + data transfer fee
- network protection on aws
  - network access control (NACL)
  - aws vpc security groups
  - aws waf
  - aws shield & aws shield advanced
  - aws firewall manager(to manage them across accounts)
  - But what if we want to protect in a sophisticated way our entire VPC?
- AWS Network Firewall
  - Protect your entire Amazon VPC
  - From Layer 3 to Layer 7 protection
  - any direction: vpc-vpc, outbound to internet, inbound from internet, To/from Direct Connect & Site-to-Site VPN
  - internally, the aws network firewall uses the aws gateway load balancer
  - Rules can be centrally managed cross- account by AWS Firewall Manager to apply to many VPCs
  - Fine Grained Controls:
    - support 1000s of rules:
      - ip & port
      - protocol
      - Stateful domain list rule groups: only allow outbound traffic to *.mycorp.com or third-party software repo
      - General pattern matching using regex
    - Traffic filtering: Allow, drop, or alert for the traffic that matches the rules
    - **Active flow inspection** to protect against network threats with intrusion- prevention capabilities (like Gateway Load Balancer, but all managed by AWS)
    - Send logs of rule matches to Amazon S3, CloudWatch Logs, Kinesis Data Firehose

## Route 53

- DNS: Domain Name System which translates the human friendly hostnames into the machine IP addresses
- DNS Terminologies:
  - Domain Registrar
  - DNS Records
  - Zone File
  - Name Server
  - Top Level Domain(TLD): .com, .us, .in,...
  - Second Level Domain (SLD): amazon.com, google.com, ...
- Amazon Route 53
  - A highly available, scalable, fully
managed and Authoritative DNS.
  - Also a domain registar
  - can do health check
- route 53 records (How you want to route traffic for a domain)
  - each record contains:
    - domain/subdomain name
    - record type:
      - A: maps a hostname to IPv4
      - AAAA: maps a hostname to IPv6
      - CNAME: maps a hostname to another hostname
        - The target is a domain name which must have an A or AAAA record
        - Can’t create a CNAME record for the top node of a DNS namespace (Zone Apex)
      - NS: Name Servers for the Hosted Zone
    - value: ip address
    - routing policy
    - ttl:
      - high ttl (such as 24hr): Less traffic on Route 53. Possibly outdated records
      - low ttl (such as 60 sec): More traffic on Route 53 ($$). easy to change records. Records are outdated for less time
      - Except for Alias records, TTL is mandatory for each DNS record
  - some advanced records: CAA, DS, MX,...
- route 53 hosted zones($0.50 per zone)
  - A container for records that define how to route traffic to a domain and its subdomains
  - Public Hosted Zones – contains records that specify how to route traffic on the Internet (public domain names)
  - Private Hosted Zones – contain records that specify how you route traffic within one or more VPCs (private domain names)
- CNAME vs Alias
  - AWS Resources (Load Balancer, CloudFront...) expose an AWS hostname
  - CNAME: Points a hostname to any other hostname. ONLY FOR NON ROOT DOMAIN
  - Alias: Points a hostname to an AWS Resource.
    - free of charge
    - native health check
    - Works for ROOT DOMAIN and NON ROOT DOMAIN
- route53 alias records
  - Maps a hostname to an AWS resource
  - An extension to DNS functionality
  - Automatically recognizes changes in the resource’s IP addresses
  - Unlike CNAME, it can be used for the top node of a DNS namespace (Zone Apex), e.g.: example.com
  - Alias Record is always of type A/AAAA for AWS resources (IPv4 / IPv6)
  - You can’t set the TTL
- route 53 alias records targets
  - elb
  - cloudfront distribution
  - api gateway
  - s3 website
  - elastic beanstalk environments
  - vpc interface endpoint
  - global accelerator
  - route 53 record in the same hosted zone
  - You cannot set an ALIAS record for an EC2 DNS name
- route 53 routing policies
  - Define how Route 53 responds to DNS queries
  - Don’t get confused by the word “Routing”: DNS does not route any traffic, it only responds to the DNS queries
  - routing policies: simple, weighted, failover, latency based, geolocation, multi-value answer, geoproximity(using Route 53 Traffic Flow feature)
- routing policy -- simple
  - Typically, route traffic to a single resource
  - Can specify multiple values in the same record
  - If multiple values are returned, a random one is chosen by the client
  - When Alias enabled, specify only one AWS resource
  - Can’t be associated with Health Checks
- routing policy -- weighted
  - Control what percentage of the requests that go to each specific resource
  - Assign each record a relative weight(weights do need to sum up to 100)
  - DNS records must have the same name and type
  - Can be associated with Health Checks
  - Use cases: load balancing between regions, testing new application versions...
  - Assign a weight of 0 to a record to stop sending traffic to a resource
  - If all records have weight of 0, then all records will be returned equally
- routing policy -- latency-based
  - Redirect to the resource that has the least latency close to us
  - Super helpful when latency for users is a priority
  - Latency is based on traffic between users and AWS Regions
  - Can be associated with Health Checks (has a failover capability)
- route 53 -- health check
  - HTTP Health Checks are only for public
resources
  - Health Checks are integrated with cloudwatch metrics:
    - monitor an endpoint: About 15 global health checkers will check the endpoint health
      - health/unhealth threshold
      - interval
      - supported protocol
      - health check pass only when the response is 2xx or 3xx
      - health check can be configured to check first 5120 bytes of the response
    - monitor other other health checks: Calculated Health Checks. can use OR, AND, or NOT. Up to 256 child health checks
    - monitor cloudwatch metrics(full control)
  - Health Check => Automated DNS Failover
- Health Checks – Private Hosted Zones
  - Route 53 health checkers are outside the VPC
  - They can’t access private endpoints (private VPC or on-premises resource)
  - You can create a CloudWatch Metric and associate a CloudWatch Alarm, then
create a Health Check that checks the alarm itself
- routing policy -- failover(active-passive)
- routing policy -- geolocation
  - Different from Latency-based!
  - This routing is based on user location
  - Specify location by Continent, Country or by US State (if there’s overlapping, most precise location selected)
  - Should create a “Default” record (in case there’s no match on location)
  - Use cases: website localization, restrict content distribution, load balancing, ...
  - Can be associated with Health Checks
- Routing Policies – Geoproximity
  - Route traffic to your resources based on the geographic location of users and resources
  - Ability to `shift more traffic to resources` based on the defined `bias`
  - To change the size of the geographic region, specify bias values:
    - To expand (1 to 99) – more traffic to the resource
    - To shrink (-1 to -99) – less traffic to the resource
  - Resources can be: aws resources or non-aws resources
  - you must use route 53 **traffic flow** to use this feature 
- route 53 -- traffic flow
  - Simplify the process of creating and maintaining records in large and complex configurations
  - **Visual editor** to manage complex routing decision trees
  - Configurations can be saved as Traffic Flow Policy:
    - Can be applied to different Route 53 Hosted Zones (different domain names)
    - Supports versioning
- routing policy -- ip-based routing
  - Routing is based on clients’ IP addresses
  - You provide a list of CIDRs for your clients and the corresponding endpoints/locations (user-IP-to-endpoint mappings)
  - Use cases: Optimize performance, reduce network costs...
  - Example: route end users from a particular ISP to a specific endpoint
- Routing Policies – Multi-Value
  - Use when routing traffic to multiple resources
  - Route 53 return multiple values/resources
  - Can be associated with Health Checks (return only values for healthy resources)
  - Up to 8 healthy records are returned for each Multi-Value query
  - Multi-Value is not a substitute for having an ELB
- Domain Registar vs. DNS Service
  - can buy one domain name from a domain registar, and manage it using route 53
  - create a hosted zone in route 53
  - Update NS Records on 3rd party website to use Route 53 Name Servers
- S3 Website with Route 53
  - Create an S3 bucket with the same name as the target record
  - Enable S3 website on the bucket (and enable S3 bucket public settings)
  - Create a Route 53 Alias record to the S3 website endpoint or type A – IPv4 address
  - This only works for HTTP traffic (for HTTPS, use CloudFront)
- Route 53 – Hybrid DNS
  - By default, Route 53 Resolver automatically answers DNS queries for: Local domain names for EC2 instances, records in private hosted zones or public name servers
  - Hybrid DNS – resolving DNS queries between VPC (Route 53 Resolver) and your networks (other DNS Resolvers)
  - Networks can be: vpc/peered vpc; on-prem network(connected through Direct Connect or AWS VPN)
- Route 53 – Resolver Endpoints
  - Inbound Endpoint
    - DNS Resolvers on your network can forward DNS queries to Route 53 Resolver
    - Allows your DNS Resolvers to resolve domain names for AWS resources (e.g., EC2 instances) and records in Route 53 Private Hosted Zones
  - Outbound Endpoint
    - Route 53 Resolver conditionally forwards DNS queries to your DNS Resolvers
    - Use Resolver Rules to forward DNS queries to your DNS Resolvers
  - Associated with one or more VPCs in the same AWS Region
  - Create in two AZs for high availability
  - Each Endpoint supports 10,000 queries per second per IP address
- Route 53 – Resolver Inbound Endpoints
- Route 53 – Resolver Outbound Endpoints
- Route 53 – Resolver Rules (outbound endpoint: forward dns queries to the dns server on-prem from your vpc)
  - Conditional Forwarding Rules (Forwarding Rules): Forward DNS queries for a specified domain and all its subdomains to target ip addresses
  - System Rules: Selectively overriding the behavior defined in Forwarding Rules (e.g., don’t forward DNS queries for a subdomain acme.example.com)
  - Auto-defined System Rules: Defines how DNS queries for selected domains are resolved (e.g., AWS internal domain names, Privated Hosted Zones)
  - If multiple rules matched, Route 53 Resolver chooses the most specific match
  - Resolver Rules can be shared across accounts using AWS RAM:
    - Manage them centrally in one account
    - Send DNS queries from multiple VPC to the target IP defined in the rule

## Other services

- x-ray: Visual analysis of our applications. Especially great for distributed systems
  - Understand dependencies in a microservice architecture
  - Review request behavior
  - Pinpoint service issues, errors, exceptions
  - Troubleshooting performance (bottlenecks, Where I am throttled?)
  - Identify users that are impacted
  - Are we meeting time SLA?
- amplify
  - A set of tools and services that helps you develop and deploy scalable full stack web and mobile applications
  - Authentication, Storage, API(REST,GraphQL), CI/CD, PubSub, Analytics, AI/MLPredictions, Monitoring, ...
  - Connect your source code from GitHub, AWS CodeCommit, Bitbucket, GitLab, or upload directly


## Tests reviews

### table of contents

- [Practice Exam Review](#practice-exam-review)
- [Practice Test1 Review](#practice-test1-review)
- [Practice Test2 Review](#practice-test2-review)
- [Practice Test3 Review](#practice-test3-review)
- [Practice Test4 Review](#practice-test4-review)



### Practice Exam Review

- An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing per-socket, per-core, or per-VM software licenses
- You can allow only SSL connections to your RDS for PostgreSQL database instance by enabling the rds.force_ssl parameter ("0" by default) through the parameter groups page on the RDS Console or through the CLI.
- By default, Amazon Simple Storage Service (Amazon S3) doesn't collect server access logs. When you enable logging, Amazon S3 delivers access logs for a source bucket to a target bucket that you choose. The target bucket must be in the same AWS Region as the source bucket and must not have a default retention period configuration. And then use Athena to analyze the logs
- All objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant time-limited permission to download the objects.
- You can set up the CloudWatch agent to use multiple configuration files. Any configuration files that will be used together on the same server must have different file names. If you use append-config with a configuration file with the same file name as a configuration file that the agent is already using, the append command overwrites the information from the first configuration file instead of appending to it.
- A stack policy applies only during stack updates, it doesn't provide access controls. The developer needs to provide access through IAM policies. It doesn't provide access controls like an AWS Identity and Access Management (IAM) policy. Use a stack policy only as a fail-safe mechanism to prevent accidental updates to specific stack resources. To control access to AWS resources or actions, use IAM.
- s3 bucket versions: Different versions of a single object can have different retention modes and periods. When you apply a retention period to an object version explicitly(or through a bucket default setting which no need for specifying `retain until date`), you specify a Retain Until Date for the object version
- CloudFormation StackSets allow you to roll out CloudFormation stacks over multiple AWS accounts and in multiple Regions with just a couple of clicks. You can now centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions.
- To control how AWS CloudFormation handles the EBS volume when the stack is deleted, set a deletion policy for your volume.
- Use an Elastic Beanstalk blue (environment A)/green (environment B) deployment to decouple the RDS DB instance from environment A. Create a new Elastic Beanstalk environment (environment B) with the necessary information to connect to the decoupled RDS DB instance
- AWS Storage Gateway uses SSL/TLS (Secure Socket Layers/Transport Layer Security) to encrypt data that is transferred between your gateway appliance and AWS storage. By default, Storage Gateway uses Amazon S3-Managed Encryption Keys to server-side encrypt all data it stores in Amazon S3
- Enable Enhanced Monitoring for your RDS DB instance. By default, Enhanced Monitoring metrics are stored for 30 days in the CloudWatch Logs, which are different from typical CloudWatch metrics. Enhanced Monitoring for RDS provides the following OS metrics: 1.Free Memory 2.Active Memory 3.Swap Free 4.Processes Running 5.File System Used
- The error indicates the IAM role is not correctly configured. After you've created a flow log, you cannot change its configuration. Instead, you need to delete the flow log and create a new one with the required configuration
- if a cloudformation stack(assuming resources are created in a private subnet) fails because it fails to receive a signal from ec2 instance, then it could be there is no network route (NAT gateway or internet gateway), or The cfn-signal script does not get executed before the timeout of the wait condition
- You can bring an existing resource into AWS CloudFormation management using resource import
- Your website is hosted on S3 and exposed through a CloudFront distribution and some users are said to experience a lot of 501 errors, then start to analyze the cloudfront access logs using Athena (cloudfront access logs can be configured to send to a s3 bucket). Do not start with s3 access logs, cuz it will not provide any details about IP or stuff, as requests are proxied through cloudfront. in addition, cloudfront will cache results, so s3 access logs will not contain much information
- The Security Groups of instances on VPC1 should be configured to allow inbound traffic from resources in VPC2. By default, Network ACLs allow all inbound and outbound traffic. So, a default Network ACLs on VPC1 will not need any configuration changes
- Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket. You can set default encryption on a bucket so that all new objects are encrypted when they are stored in the bucket. There is no change to the encryption of the objects that existed in the bucket before default encryption was enabled.
- for rds, You can only enable encryption for an Amazon RDS DB instance when you create it, not after the DB instance is created.
- for ebs, There is no direct way to encrypt an existing unencrypted volume or snapshot, you can encrypt them by creating either a volume or a snapshot.
- for efs, You can enable encryption of data at rest when creating an Amazon EFS file system. Once the file system is created, you cannot modify the file system to be unencrypted or vice-versa.
- The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space.
- To support throughput near or exceeding 20K packets per second (PPS) on the VIF driver. Enhanced networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types. SR-IOV is a method of device virtualization that provides higher I/O performance and lower CPU utilization when compared to traditional virtualized network interfaces. Enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies. There is no additional charge for using enhanced networking.
- to share CMK across accounts, The key policy for the CMK must give the external account (or users and roles in the external account) permission to use the CMK. IAM policies in the external account must delegate the key policy permissions to its users and roles
- S3 Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.
- When you want to move your application from one instance store-backed instance to an instance store-backed instance with a different instance type, you must migrate it by creating an image from your instance, and then launching a new instance from this image with the instance type that you need.
- An instance store-backed EC2 instance can be resized
- If an IAM user, with full access to IAM and Amazon S3, assigns a bucket policy to an Amazon S3 bucket and doesn't specify the AWS account root user as a principal, the root user is denied access to that bucket. However, as the root user, you can still access the bucket. To do that, modify the bucket policy to allow root user access from the Amazon S3 console or the AWS CLI.
- If the load balancer is not responding to client requests:
  - A security group or network ACL does not allow traffic
  - Your internet-facing load balancer may be attached to a private subnet
- Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. (80TB). You can't directly copy data from Snowball Edge devices into AWS Glacier. The data stored on the Snowball Edge device can be copied into the S3 bucket and later transitioned into AWS Glacier via a lifecycle policy.
- NetworkIn and NetworkOut - You can determine which instance is causing high network usage using the Amazon CloudWatch NetworkIn and NetworkOut metrics. You can aggregate the data points from these metrics to calculate the network usage for your instance.
- After enabling S3 MFA-Delete, the actions that need MFA:
  - Suspending versioning
  - Permanently delete an object version
- cloudformation: Set OnFailure=DO_NOTHING. You can use the OnFailure property of the CloudFormation CreateStack call for this use-case. The OnFailure property determines what action will be taken if stack creation fails. This must be one of DO_NOTHING, ROLLBACK, or DELETE. You can specify either OnFailure or DisableRollback, but not both.
- ebs volume vs instance store:
  - By default, data on a non-root EBS volume is preserved even if the instance is shutdown or terminated
  - EBS snapshots only capture data that has been written to your Amazon EBS volume, which might exclude any data that has been locally cached by your application or operating system
  - As a best practice, AWS recommends the use of separate Amazon EBS volumes for the operating system and your data. This ensures that the volume with your data persists even after instance termination or any issues to the operating system.
- A Golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc.
- elastic beanstalk: Create a new beanstalk environment for each application and apply blue/green deployment patterns. Swap CNAMEs of the two environments
- ec2 termination protection:
  - You can't enable **termination protection** for Spot Instances (the `DisableApiTermination` attribution)
  - To prevent instances that are part of an Auto Scaling group from terminating on scale in, use **instance protection** instead of Amazon EC2 termination protection
- If your AMI contains a CloudWatch agent, it’s automatically installed on EC2 instances when you create an EC2 Auto Scaling group. With the stock Amazon Linux AMI, you need to install it (AWS recommends to install via yum).
- Add a common tag to each bucket. Activate the tag as a cost allocation tag. Use the AWS Cost Explorer to create a cost report for the tag. Start by adding a common tag to each bucket. Activate the tag as a cost allocation tag. Use the AWS Cost Explorer to create a cost report for the tag. After you create the cost report, you can use it to review the cost of each bucket that has the cost allocation tag that you created.
- s3 cost report: You can set up a daily or hourly AWS Cost and Usage report to get more Amazon S3 billing details. However, these reports won't show you who made requests to your buckets. To get more information on certain Amazon S3 billing items, you must enable logging ahead of time. Then, you'll have logs that contain Amazon S3 request details.
- Check that the device name you specified when you attempted to attach the EBS volume isn't already in use. Attempt to attach the volume to the instance, again, but use a different device name
- Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket. You can set default encryption on a bucket so that all new objects are encrypted when they are stored in the bucket. The objects are encrypted using server-side encryption with either Amazon S3-managed keys (SSE-S3) or customer master keys (CMKs) stored in AWS Key Management Service (AWS KMS).
- It isn't possible to restore or recover a deleted or deregistered AMI. However, you can create a new, identical AMI using one of the following:
  - Amazon Elastic Block Store (Amazon EBS) snapshots that were created as backups
  - Amazon Elastic Compute Cloud (Amazon EC2) instances that were launched from the deleted AMI
- Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.
- StatusCheckFailed: The AWS/EC2 namespace includes a few status check metrics. By default, status check metrics are available at a 1-minute frequency at no charge. Reports whether the instance has passed both the instance status check and the system status check in the last minute. This metric can be either 0 (passed) or 1 (failed).
- Create the AMI by disabling the No reboot option - On the Create image page, No reboot flag is present. The default functionality is, Amazon EC2 shuts down the instance, takes snapshots of any attached volumes, creates and registers the AMI, and then reboots the instance. When No reboot option is selected, the instance is not shut down while creating the AMI. This option is not selected by default.
- Create the AMI with No reboot option enabled - If the No reboot flag is selected, the instance is not shutdown while creating an AMI. This implies, the Operating System buffers are not flushed before creating an AMI, so data integrity could be an issue with AMIs created in this way. Such AMIs are crash-consistent but not application-consistent.
- Configure CloudFront to mandate viewers to use HTTPS to request objects from S3. CloudFront and S3 will use HTTP to communicate with each other. If your Amazon S3 bucket is configured as a website endpoint, you can't configure CloudFront to use HTTPS to communicate with your origin because Amazon S3 doesn't support HTTPS connections in that configuration.
- when your origin is an amazon s3 bucket that supports https communication, cloudfront always forwards requests to s3 by using the protocol that views used to submit requests.
- To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file of your Beanstalk environment
- asg & elb health check:
  - ec2 health check: Suitable for situations where you primarily need to ensure that your EC2 instances are operational and not facing hardware issues.
  - elb health check: Recommended when you need to ensure that your application is truly available to handle traffic, as it provides a deeper level of health awareness.
- You can use the wait condition handle to make AWS CloudFormation pause the creation of a stack and wait for a signal before it continues to create the stack. For example, you might want to download and configure applications on an Amazon EC2 instance before considering the creation of that Amazon EC2 instance complete.
- the cfn-signal command is managed via CloudFormation, not via the user data.
- With Amazon EBS, you can use any of the standard RAID configurations that you can use with a traditional bare metal server, as long as that particular RAID configuration is supported by the operating system for your instance. This is because all RAID is accomplished at the software level. For greater I/O performance than you can achieve with a single volume, RAID 0 can stripe multiple volumes together; for on-instance redundancy, RAID 1 can mirror two volumes together.
- If you attempt to delete a stack with termination protection enabled, the deletion fails and the stack - including its status - remains unchanged
- The Auto Scaling group in your Elastic Beanstalk environment uses two default Amazon CloudWatch alarms to trigger scaling operations. These alarms must be configured based on the parameters appropriate for your application. Default Auto Scaling triggers are configured to scale when the average outbound network traffic (NetworkOut) from each instance is higher than 6 MB or lower than 2 MB over a period of five minutes. For more efficient Amazon EC2 Auto Scaling, configure triggers that are appropriate for your application, instance type, and service requirements. You can scale based on several statistics including latency, disk I/O, CPU utilization, and request count.
- when enabling s3 bucket versioning, GET requests do not retrieve delete marker objects. The only way to list delete markers (and other versions of an object) is by using the versions subresource in a GET Bucket versions request. A simple GET does not retrieve delete marker objects.
- Elastic Load Balancing is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Elastic Load Balancing. CloudTrail captures all API calls for Elastic Load Balancing as events. The calls captured include calls from the AWS Management Console and code calls to the Elastic Load Balancing API operations.
- Create a backup plan in AWS Backup. Assign tags to resources based on the environment ( Production, Development, Testing). Create one backup policy for production environments and one backup policy for non-production environments. Schedule the backup plan based on the organization's backup policies
- instance store-backed vs ebs-backed
  - ebs-backed: If the root device for your instance is an EBS volume, you can change the size of the instance simply by changing its instance type, which is known as resizing it.
  - instance store-backed: If the root device for your instance is an instance store volume, you must migrate your application to a new instance with the instance type that you need.
  - if your instances are in an asg, you need to suspend the scaling processes for the group while you are resizing your instance, because asg will mark the stopped instances as unhealthy and replace them.
  - for ebs-backed, after resizing, the instance id does not change, cuz aws just moved the instance to new hardware
- AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. Storage Gateway provides a standard set of storage protocols such as iSCSI, SMB, and NFS, which allow you to use AWS storage without rewriting your existing applications. It provides low-latency performance by caching frequently accessed data on-premises, while storing data securely and durably in Amazon cloud storage services. Storage Gateway optimizes data transfer to AWS by sending only changed data and compressing data.
- Create identity-based IAM policy in the Finance account that allows the user to make a request to the S3 buckets in the HR and Audit accounts. Also, create resource-based IAM policies in the HR, Audit accounts that will allow the requester from the Finance account to access the respective S3 buckets
- You store your data in Amazon S3 Glacier as archives. Archives may be further grouped into vaults. S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy.
- Application Load balancers do not send data to X-Ray - Elastic Load Balancing application load balancers add a trace ID to incoming HTTP requests in a header named X-Amzn-Trace-Id. Load balancers do not send data to X-Ray and do not appear as a node on your service map.
- Application Load balancers:
  - Before you start using your Application Load Balancer, you must add one or more listeners
  - You configure target groups of an ALB by attaching them to the listeners
  - When you create a listener, you define actions for the default rule. Default rules can't have conditions. If the conditions for none of a listener's rules are met, then the action for the default rule is performed.
- Store the files in S3 and distribute them using a CloudFront distribution instead. S3 is more cost-effective than EFS.
- AWS can schedule events for your instances, such as a reboot, stop/start, or retirement. These events do not occur frequently. If one of your instances will be affected by a scheduled event, AWS sends an email to the email address that's associated with your AWS account before the scheduled event. The email provides details about the event, including the start and end date.
  - Scheduled events are managed by AWS; you cannot schedule events for your instances. 
- MFA Delete represents another layer of security wherein you can configure a bucket to enable MFA (multi-factor authentication) Delete, which requires additional authentication for either of the following operations:
  - Change the versioning state of your bucket
  - Permanently delete an object version
  - only the bucket owner (root account) can enable MFA Delete only via the AWS CLI.
- ebs snapshots are only available through aws ec2 api not s3 api, even though it is stored in s3


### Practice Test1 Review

- for elastic beanstalk, to detach a database from one environment and attach it to another environment, make a snapshot of db in environment A, then go to db console(RDS) to enable `deletion protection` to safeguard db. lastly, create a new identical environment B which connects to the same db, and then perform blue/green deployment(or CNAME swap)
- To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file of your Beanstalk environment. By default, the health check configuration of your Auto Scaling group is set as an EC2 type that performs a status check of EC2 instances.
- for ec2 health check (for an elastic beanstalk environment, the asg ec2 health check is the default setting)
  - it only checks the ec2 instances health, not the applications, servers, or docker containers
  - if your app crashes, the load balancer will remove it from the target, but asg won't replace it with a new one
- CloudFormation StackSets allow you to roll out CloudFormation stacks over multiple AWS accounts and in multiple Regions with just a couple of clicks. When AWS launched StackSets, grouping accounts was primarily for billing purposes. Since the launch of AWS Organizations, you can centrally manage multiple AWS accounts across diverse business needs including billing, access control, compliance, security and resource sharing.
- Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS.
- Stack policies help protect critical stack resources from unintentional updates that could cause resources to be interrupted or even replaced. A stack policy is a JSON document that describes what update actions can be performed on designated resources.
- AWS Systems Manager Patch Manager automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. Patch Manager provides options to scan your instances and report compliance on a schedule, install available patches on a schedule, and patch or scan instances on demand whenever you need to.
- AWS system manage automation does not include patch management
- VPC:
  - Regardless of the type of subnet, the internal IPv4 address range of the subnet is always private
  - When you create a VPC, you must specify a range of IPv4 addresses for the VPC in the form of a Classless Inter-Domain Routing (CIDR) block
  - By default, all subnets can route between each other, whether they are private or public in a VPC, the main route table facilitates this communication
  - subnets must reside in one AZ, cannot span across multi-az
- ec2 metrics:
  - CPUUtilization metric should be used to identify the processing power required
  - CPUCreditUsage metric identifies the number of CPU credits spent by the instance for CPU utilization.
  - ResourceCount metric defines the number of the specified resources running in your account. The resources are defined by the dimensions associated with the metric.
- StatusCheckFailed - Reports whether the instance has passed both the instance status check and the system status check in the last minute. This metric can be either 0 (passed) or 1 (failed). By default, this metric is available at a 1-minute frequency at no charge.
- Monitor Trusted Advisor service check results with Amazon CloudWatch Events - AWS Trusted Advisor checks for service usage that is more than 80% of the service limit. You can use Amazon CloudWatch Events to detect and react to changes in the status of Trusted Advisor checks. Then, based on the rules that you create, CloudWatch Events invokes one or more target actions when a status check changes to the value you specify in a rule. 
- CloudWatch ServiceLens enhances the observability of your services and applications by enabling you to integrate traces, metrics, logs, and alarms into one place. So, ServiceLens can be used once we define the alarms in CloudWatch, not without it.
- AWS inspector: lambda, ec2, ecr. network accessibility, image vulnerability, security
- For Lambda functions configured as a target to EventBridge, you need to provide resource-based policy. IAM Roles will not work - IAM roles for rules are only used for events related to Kinesis Streams. For Lambda functions and Amazon SNS topics, you need to provide resource-based permissions.
  - for lambda, sqs, sns, cloudwatch logs, eventbridge relis on resource-based policies
  - for kinesis streams, eventbridge relis on iam roles.
- To configure your Auto Scaling group to scale based on a schedule, you create a `scheduled action`. The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action.
- If your AMI contains a CloudWatch agent, it’s automatically installed on EC2 instances when you create an EC2 Auto Scaling group. With the stock Amazon Linux AMI, you need to install it (AWS recommends to install via yum).
- Status checks are built into Amazon EC2, so they **cannot** be disabled or deleted. Status checks are performed every minute, returning a pass or a fail status.
- when asg cannot launch an ec2 instance because of lack of permission to access the customer-managed CMK used to encrypt the **EBS volume**, which would an error: `Client.InternalError: Client error on launch`, then there are two possible solutions:
  - Use a CMK in the same AWS account as the Auto Scaling group. Copy and re-encrypt the snapshot with another CMK that belongs to the same account as the Auto Scaling group. Allow the service-linked role to use the new CMK.
  - Continue to use the CMK in a different AWS account from the Auto Scaling group. Determine which service-linked role to use for this Auto Scaling group. Allow the Auto Scaling group account access to the CMK. Define an IAM user or role in the Auto Scaling group account that can create a grant. Create a grant to the CMK with the service-linked role as the grantee principal. Update the Auto Scaling group to use the service-linked role.
    - **Note**: if the cmk and asg are in the same aws account, then just need to update the key policy of the cmk to allow the service-linked role to the use the cmk, and then update the asg to use the service-linked role.
- You can set up the CloudWatch agent to use multiple configuration files. For example, you can use a common configuration file that collects a set of metrics and logs that you always want to collect from all servers in your infrastructure. You can then use additional configuration files that collect metrics from certain applications or in certain situations. To set this up, first create the configuration files that you want to use. Any configuration files that will be used together on the same server **must have different file names**. You can store the configuration files on servers or in Parameter Store.
  - `fetch-config`: start using cloudwatch agent and specify the first configuration file
  - `append-config`: to append any extra configuration files (different names, or it will overwrites the first configuration file)
- cloudformation:
  - !GetAtt: returns the value of an attribute from a resource in the template.
  - !Sub: substitutes variables in an input string with values that you specify.
  - !Ref: returns the value of the specified parameter or resource
  - !FindInMap: returns the value corresponding to keys in a two-level map
- AWS Storage Gateway uses SSL/TLS (Secure Socket Layers/Transport Layer Security) to encrypt data that is transferred between your gateway appliance and AWS storage. By default, Storage Gateway uses Amazon S3-Managed Encryption Keys to server-side encrypt all data it stores in Amazon S3
- If CloudFront requests an object from your origin, and the origin returns an HTTP 4xx or 5xx status code, there's a problem with communication between CloudFront and your origin. Your CloudFront distribution might send error responses with HTTP status code 400 Bad Request, and a message similar to the following: `The authorization header is malformed; the region <AWS Region> is wrong; expecting <AWS Region>`.
  - which indicates that the cloudfront distribution could not find the origin, trying to update the cloudfront distribution to find the correct origin
- If the root device for your instance is an EBS volume, you can change the size of the instance simply by changing its instance type, which is known as resizing it. If the root device for your instance is an instance store volume, you must migrate your application to a new instance with the instance type that you need.
  - **Note**: when the ec2 instance is in the asg, remember to suspend the scaling processes while resizing the instance
- Configure the "Retain Until Date" in the object lock settings to a date that is 5 years from the object creation date and create a lifecycle policy to delete the object 5 years after the object is created.
  - **Note**: You can place a retention period on an object version either explicitly or through a bucket default setting. When you apply a retention period to an object version explicitly, you specify a `Retain Until Date` for the object version. Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires.
- You can use Amazon CloudWatch Synthetics to create canaries, configurable scripts that run on a schedule, to monitor your endpoints and APIs. Canaries follow the same routes and perform the same actions as a customer, which makes it possible for you to continually verify your customer experience even when you don't have any customer traffic on your applications.
- AWS Directory Services is a managed service that automatically creates an AWS security group in your VPC with network rules for traffic in and out of AWS managed domain controllers. The default inbound rules allow traffic from any source (0.0.0.0/0) to ports required by Active Directory. These rules do not introduce security vulnerabilities, as traffic to the domain controllers is limited to traffic from your VPC, other peered VPCs, or networks connected using AWS Direct Connect, AWS Transit Gateway or Virtual Private Network.
- Subnets inside a VPC can communicate with each other without the need for any further configuration. Hence, no additional configurations are needed.
  - The first entry in the Main route table is the default entry for local routing in the VPC; this entry enables the instances (potentially belonging to different subnets) in the VPC to communicate with each other.
- Install CloudWatch Agent on all the instances and attach an IAM role to the EC2 instances to be able to run the CloudWatch agent.
  - You must attach the CloudWatchAgentServerRole IAM role to the EC2 instance to be able to run the CloudWatch agent on the instance. This role enables the CloudWatch agent to perform actions on the instance.
  - also, in order to access aws cloudwatch to sending metrics data, we need to grant permissions for the ec2 instance
- when using the same cloudformation template to create multiple stacks in different regions, some unintended behaviors may occur, especially if the template contains some custom named iam resources. Because the iam resources must be globally unique  within your account.
- when creating an environment using elastic beanstalk, the default auto scaling triggers are configured based on two aws cloudwatch alarms, the `network-in`,`network-out`. or can configure triggers to react to statistics
- when the rds read replicas are running into issues consistently:
  - writing to tables on a read replica can break the replication
  - If the value for `the max_allowed_packet` parameter for a read replica is less than the `max_allowed_packet` parameter for the source DB instance, replica errors occur. The `max_allowed_packet` parameter is a custom parameter that you can set in a DB parameter group. The `max_allowed_packet` parameter is used to specify the maximum size of data manipulation language (DML) that can be run on the database. If the `max_allowed_packet` value for the source DB instance is larger than the `max_allowed_packet` value for the read replica, the replication process can throw an error and stop replication.
- Because of security constraints that mandate such secrets never be shared between multiple parties, AWS MFA cannot support the use of your existing Gemalto device.
- Enhanced networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types.
- if an aurora cluster is created in a single az, then if the az is down, then the only way to bring a new primary instance online is to provision a new db instance in another az
- A Systems Manager Automation document defines the Automation workflow like restarting ec2 instances, creating AMIs (the actions that Systems Manager performs on your managed instances and AWS resources).
  - Use the `AWSSupport-ExecuteEC2Rescue` document to recover impaired instances. 
- Capacity Reservations:
  - no billing discounts, but you can combine with `saving plans` or `regional reserved instances` to receive a discount
  - enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration
- An interface VPC endpoint (interface endpoint) enables you to connect to services powered by AWS PrivateLink, a technology that enables you to privately access Amazon EC2 and Systems Manager APIs by using private IP addresses.
- You can only share AMIs that have unencrypted volumes and volumes that are encrypted with a customer-managed CMK
- You do not need to share the Amazon EBS snapshots that an AMI references in order to share the AMI
- Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.
- Change sets allow you to preview how proposed changes to a stack might impact your existing resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set.
- if an iam user has a s3 bucket with a bucket policy which does not include the root user, then the root user will not be able to access the bucket, but the root user still can manage the bucket policy to grant itself permission to access the bucket
- when attaching an ebs volume, check if the device name is being used or not, otherwise try another name. when the ebs volume is stuck with attaching status, when try to force detach, and then attach with a new name
- To track requests for access to your bucket, you can enable server access logging. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant.
  - **note**: we can also use cloudtrail to identify the requests made to s3, but s3 access logging is free service
- Create a Role in production account, that defines the Development account as a trusted entity and specify a permissions policy that allows trusted users to update the bucket. Then, modify the IAM group policy in development account, so that testers are denied access to the newly created role. Developers can use the newly created role to access the live S3 buckets in production environment
- An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses.
- Cloudfront & s3:
  - **Note**: if you use an Amazon S3 bucket configured as a website endpoint, you can’t use the origin access identity feature.
  - If you use an Amazon S3 bucket configured as a website endpoint, you must set it up with CloudFront as a custom origin. You can’t use the origin access identity feature. However, you can restrict access to content on a custom origin by setting up custom headers and configuring your origin to require them:
    - Origin Custom Headers: Configure CloudFront to forward custom headers to your origin.
    - Viewer Protocol Policy: Configure your distribution to require viewers to use HTTPS to access CloudFront.
    - Origin Protocol Policy: Configure your distribution to require CloudFront to use the same protocol as viewers to forward requests to the origin.
- After you increase the size of an EBS volume, you must use the file-system specific commands to extend the file system to the larger size. You can resize the file system as soon as the volume enters the optimizing state.
- on ec2 instances, when collecting custom metrics using cloudwatch agent, you can use `StatsD` and `collectd` protocols
  - `StatsD`: supported on both Linux servers and servers running Windows Server.
  - `collectd`: supported only on Linux servers. 
- Alarms continue to evaluate metrics against your chosen threshold, even after they have already triggered. This allows you to view its current up-to-date state at any time. You may notice that one of your alarms stays in the ALARM state for a long time. If your metric value is still in breach of your threshold, the alarm will remain in the ALARM state until it no longer breaches the threshold. This is normal behavior. If you want your alarm to treat this new level as OK, you can adjust the alarm threshold accordingly.
- If you need to remove a file from CloudFront edge caches before it expires, you can do one of the following:
  - Invalidate the file from edge caches.
  - Use file versioning to serve a different version of the file that has a different name.
- You can use the CloudFront console to create and run an invalidation, display a list of the invalidations that you submitted previously, and display detailed information about an individual invalidation. You can also copy an existing invalidation, edit the list of file paths, and run the edited invalidation. You can't remove invalidations from the list.
- When you submit an invalidation request to CloudFront, CloudFront forwards the request to all edge locations within a few seconds, and each edge location starts processing the invalidation immediately. As a result, you can’t cancel an invalidation after you submit it.
- If you created an AWS resource outside of AWS CloudFormation management, you can bring this existing resource into AWS CloudFormation management using `resource import`.
- Create identity-based IAM policy in the Finance account that allows the user to make a request to the S3 buckets in the HR and Audit accounts. Also, create resource-based IAM policies in the HR, Audit accounts that will allow the requester from the Finance account to access the respective S3 buckets
- Traffic between EC2 instances in different AWS Regions stays within the AWS network, if there is an Inter-Region VPC Peering connection between the VPCs where the two instances reside
- Traffic between two EC2 instances in the same AWS Region stays within the AWS network, even when it goes over public IP addresses
- By default, you can terminate your instance using the Amazon EC2 console, command line interface, or API. But you can enable termination protection for the instance. The `DisableApiTermination` attribute controls whether the instance can be terminated using the console, CLI, or API.
  - **note**: however, the `DisableApiTermination` attribute does not prevent you from terminating an instance by initiating shutdown from the instance (using an operating system command for system shutdown) when the `InstanceInitiatedShutdownBehavior` attribute is set(stop, terminate, hibernate).
- In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone.
- To control whether an Auto Scaling group can terminate a particular instance when scaling in, use instance **scale-in protection**. You can enable the instance scale-in protection setting on an `Auto Scaling group` or an `individual Auto Scaling instance`.
  - the **scale-in protection** does not protect instances from:
    - manual termination. (to protect against manual termination, enable ec2 termination protection)
    - health check replacement. (to protect against unhealthy termination, suspend the `ReplaceUnhealthy` process)
    - spot instance interruptions
- Create the AMI by disabling the `No reboot` option
- use aws config to define rules and carry out the necessary `auto-remediation`(aws config rule feature) if needed
- If there is at least one healthy target in a target group, the load balancer routes requests only to the healthy targets. If a target group contains only unhealthy targets, the load balancer routes requests to the unhealthy targets. Hence, it is advised to configure an Auto Scaling Group, if the instances are hosting a business-critical application.


### Practice Test2

- S3 is more cost-effective than EFS. and deliver content out of cloudfront can be more cost-effective than delivering it from s3 bucket
- To perform queries, you can connect to the `reader endpoint`, with Aurora automatically performing load-balancing among all the Aurora Replicas.
- Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket. You can set default encryption on a bucket so that all new objects are encrypted when they are stored in the bucket. The objects are encrypted using server-side encryption with either Amazon S3-managed keys (SSE-S3) or customer master keys (CMKs) stored in AWS Key Management Service (AWS KMS).
- `InsufficientInstanceCapacity` error implies that AWS does not have the capacity to serve your request.
- `InstanceLimitExceeded` error implies that you have reached the limit on the number of instances that you can launch in a region
- AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices. Whether establishing new workflows, developing applications, or as part of ongoing improvement, you can take advantage of the recommendations provided by Trusted Advisor regularly to help keep your solutions provisioned optimally.
- AWS Organizations helps you to centrally manage billing; control access, compliance, and security; and share resources across your AWS accounts. Using AWS Organizations, you can automate account creation, create groups of accounts to reflect your business needs, and apply policies for these groups for governance.
- The OnFailure property determines what action will be taken if stack creation fails. This must be one of DO_NOTHING, ROLLBACK, or DELETE. You can specify either OnFailure or DisableRollback, but not both.
- after enabling s3 MFA-delete,
  - suspending versioning needs MFA
  - permanently delete an object version needs MFA
- Federate the users with Cognito so they can assume a role to access S3
  - user pool: a user directory
  - identity pool: users can obtain tmeporary aws credentials to access aws services
- File Gateway provides a seamless way to connect to the cloud in order to store application data files and backup images as durable objects in Amazon S3 cloud storage. File Gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. It can be used for on-premises applications, and for Amazon EC2-based applications that need file protocol access to S3 object storage.
- for Elastic load balancer, it publishes metrics to cloudwatch in 60-second intervals. and it has a metric `ActiveConnectionCount` metric to monitor the total number of concurrent tcp connections active from clients
- With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. and then AWS Systems Manager patch manager helps you select and deploy operating system and software patches automatically across large groups of Amazon EC2 or on-premises instances.
- Patch Manager, a capability of AWS Systems Manager, can be used to automate patch management and OS updates for all the instances at one go
- `SpilloverCount` represents the total number of requests that were rejected because the `surge queue` is full. The `SurgeQueueLength` is the metric of `Classic load balancer`, which measures the total number of requests queued by your clb. and this metric could mean the backend services or systems could not take all incoming requests and process them. to solve this issue, we can configure asg to scale based on the `SurgeQueueLength`. these two metrics are the metrics of clb
- You can allow only SSL connections to your RDS for PostgreSQL database instance by enabling the rds.force_ssl parameter ("0" by default) through the `parameter groups` page on the RDS Console or through the CLI.
- ELB access logs is an optional feature of Elastic Load Balancing that is disabled by default. The access logs capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.
- You can use a network address translation (NAT) gateway or internet gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances.
- To control how AWS CloudFormation handles the EBS volume when the stack is deleted, set a deletion policy for your volume. You can choose to retain the volume, to delete the volume, or to create a snapshot of the volume.
- you need to add a route with a target of 0.0.0.0/0 to the NAT Gateway to make sure the instances in the private subnet to access the internet.
- Tape Gateway enables you to replace using physical tapes on-premises with virtual tapes in AWS without changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on-premises for low-latency data access.
- You should note that only the bucket owner (root account) can enable MFA Delete only via the AWS CLI. However, the bucket owner, the AWS account that created the bucket (root account), and all authorized IAM users can enable versioning.
- GuardDuty is a threat detection service that monitors malicious activity and unauthorized behavior to protect your AWS account. GuardDuty analyzes billions of events across your AWS accounts from AWS CloudTrail (AWS user and API activity in your accounts), Amazon VPC Flow Logs (network traffic data), and DNS Logs (name query patterns). 



### Practice Test3
### Practice Test4











